{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/URBAN-IA/URBANIA/blob/main/study/DL/Deep_Learning_%E1%84%89%E1%85%A5%E1%86%BC%E1%84%82%E1%85%B3%E1%86%BC_%E1%84%80%E1%85%A9%E1%84%83%E1%85%A9%E1%84%92%E1%85%AA_%E1%84%87%E1%85%A1%E1%86%BC%E1%84%87%E1%85%A5%E1%86%B8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKjg766IvMdI"
      },
      "source": [
        "<img src=\"https://i.imgur.com/Ur83eQl.png\"></img>\n",
        "\n",
        "# (3-5) 성능 고도화 방법 실습"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2xz20jHFVni"
      },
      "source": [
        "## 실습 개요\n",
        "\n",
        "<hr style=\"height:5px;border:none;color:#5F71F7;background-color:#5F71F7\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnGG7TUnFVni"
      },
      "source": [
        "### 1) 실습 목적\n",
        "이번 실습에서는 강의에서 배웠던 다양한 딥러닝 성능 고도화 방법를 구현해봅니다. 이전 실습과 동일하게 MNIST를 이용하여 직접 NumPy로 신경망을 만들어 학습시키고 드롭아웃(Dropout), 가중치 초기화(Weight initialization) 그리고 배치 정규화(Batch Normalization)와 같은 성능 고도화 방법들을 구현하고 이를 직접 확인해보는 시간을 가져봅시다. 나아가 SGD부터 Adam에 이르기까지 다양한 옵티마이저(Optimizer)를 구현해봅니다.\n",
        "\n",
        "### 2) 수강 목표\n",
        "- 성능 고도화 방법 중 하나인 드롭아웃(Dropout)에 대해서 이해하고 구현할 수 있다.\n",
        "- 딥러닝에서 대표적인 정규화 방법 중 배치 정규화(Batch Normalizaiton)에 대해서 이해하고 구현할 수 있다.\n",
        "- SGD부터 Adam에 이르기까지 딥러닝에서 가장 많이 사용되는 옵티마이저(Optimizer)의 동작 원리를 이해하고 직접 구현할 수 있다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jC9GdsvPFVnj"
      },
      "source": [
        "### 실습 목차\n",
        "1. [복습] 신경망 학습하기\n",
        "2. 드롭아웃(Dropout)\n",
        "3. 배치 정규화(Batch Normalization)\n",
        "4. 딥러닝에서의 다양한 옵티마이저(Optimizer)\n",
        "    - 4-1. SGD 이해 및 구현\n",
        "    - 4-2. Momentum 이해 및 구현\n",
        "    - 4-3. Nesterov 이해 및 구현\n",
        "    - 4-4. AdaGrad 이해 및 구현\n",
        "    - 4-5. RMSProp 이해 및 구현\n",
        "    - 4-6. Adam 이해 및 구현\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 실습 구성\n",
        "> 앞으로 여러분들이 마주하게 될 실습코드는 다음과 같이 3개의 종류로 구성되어 있습니다.\n",
        "\n",
        "- 📝 <font color='orange'><b>[ 설명 ]</b></font> : 코드를 작성하기 전 필요한 지식\n",
        "- 👨‍💻 <font color='green'><b>[ 코드 ]</b></font>  : 본격적인 코드 작성\n",
        "- 📚 <font color='blue'><b>[ 자료 ]</b></font>  : 코드와 관련한 읽을거리"
      ],
      "metadata": {
        "id": "IUQYaE_uN0-C"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yeR6KBXJFVnj"
      },
      "source": [
        "### 데이터셋 개요\n",
        "\n",
        "<div align=\"center\">\n",
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/f/f7/MnistExamplesModified.png\" alt=\"MNIST Test Data Samples\">\n",
        "</div>\n",
        "\n",
        "\n",
        "* 데이터셋 :\n",
        "MNIST(Modified National Institute of Standards and Technology) Database\n",
        "\n",
        "* 데이터셋 개요 :\n",
        "<br>&ensp;&ensp;MNIST 데이터셋은 미국의 NIST에서 이미지 처리 시스템을 위해 모은 손글씨 이미지 데이터셋 NIST Special Database 3 중 일부를 재구성한 것으로, 0부터 9까지의 숫자 이미지와 이에 대응되는 숫자가 한 쌍으로 구성되어 있습니다. 이 데이터셋은 머신러닝 및 딥러닝 분야에서 가장 잘 알려진 벤치마크 데이터셋 중 하나로, 특히 딥러닝 기초 관련 교보재에 주로 활용되고 있습니다.\n",
        "\n",
        "  - 데이터셋 구성\n",
        "    - 입력(이미지) : 0부터 9까지의 숫자 중 하나에 속하는 64px * 64px의 흑백 이미지\n",
        "    - 출력(숫자) : 주어진 이미지에 대응되는 숫자 (0~9)\n",
        "  - 데이터 샘플 수\n",
        "    - 학습 데이터&ensp;&ensp;: 60,000 개\n",
        "    - 테스트 데이터&thinsp;: 10,000 개\n",
        "\n",
        "* 데이터셋 저작권: [CC BY-SA 3.0](https://creativecommons.org/licenses/by-sa/3.0/)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ssa28zPcFVnj"
      },
      "source": [
        "### 환경 설정\n",
        "해당 실습은 Google Colab을 기준으로 제작되었습니다. 실습에서 필요한 패키지 정보는 아래와 같습니다.\n",
        "```\n",
        "numpy >= 1.23.5\n",
        "pandas >= 1.5.3\n",
        "matplotlib >= 3.7.1\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gFHNg3nFVnj"
      },
      "source": [
        "## 1. [복습] 신경망 학습하기\n",
        "\n",
        "<hr style=\"height:5px;border:none;color:#5F71F7;background-color:#5F71F7\">\n",
        "\n",
        "성능 고도화 방법에 대해서 알아보기 전에, 이전 실습에서 배웠던 내용을 다시 복습해봅시다."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 실습 환경 설정"
      ],
      "metadata": {
        "id": "L8ge6sBKZCVH"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocTDHNkSFVnk"
      },
      "source": [
        "#### 👨‍💻 <font color='green'><b>[ 코드 ]</b></font> 라이브러리 및 옵션"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-08-27T16:16:01.883692Z",
          "start_time": "2023-08-27T16:16:01.571783Z"
        },
        "id": "s-ITfPpwFVnk"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import gzip\n",
        "import pickle\n",
        "import random\n",
        "import platform\n",
        "import warnings\n",
        "import datetime\n",
        "import numpy as np\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import OrderedDict\n",
        "from urllib.request import urlretrieve"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "실습을 위해서 데이터셋 폴더를 생성하고 여기로 Google Colab을 마운트합니다."
      ],
      "metadata": {
        "id": "wCXRRuPwIm4q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "id": "U8ZUeMUqF5Pu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-08-27T16:16:01.889726Z",
          "start_time": "2023-08-27T16:16:01.885434Z"
        },
        "id": "kAKN69cIFVnk"
      },
      "outputs": [],
      "source": [
        "# 재현성을 위해 랜덤 시드를 고정\n",
        "SEED = 2023\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
        "\n",
        "# 현재 OS 및 라이브러리 버전 체크 체크\n",
        "current_os = platform.system()\n",
        "print(f\"Current OS: {current_os}\")\n",
        "print(f\"Python Version: {platform.python_version()}\")\n",
        "\n",
        "# 중요하지 않은 에러 무시\n",
        "warnings.filterwarnings(action='ignore')\n",
        "\n",
        "# 유니코드 깨지는 현상 해결\n",
        "mpl.rcParams['axes.unicode_minus'] = False"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MNIST 데이터셋 가져오기"
      ],
      "metadata": {
        "id": "yHcEKh_jY-fd"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCb1z7UuFVnl"
      },
      "source": [
        "#### 👨‍💻 <font color='green'><b>[ 코드 ]</b></font> MNIST 데이터셋 다운로드 및 압축 해제"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# MNIST를 다운받을 경로\n",
        "url = 'http://yann.lecun.com/exdb/mnist/'\n",
        "\n",
        "# MNIST를 저장할 디렉토리 (`./data/`)\n",
        "dataset_dir = os.path.join(os.getcwd(), 'data')\n",
        "\n",
        "# Pickle로 저장할 경로\n",
        "save_file = dataset_dir + \"/mnist.pkl\"\n",
        "\n",
        "# MNIST 데이터셋의 파일명 (딕셔너리)\n",
        "key_file = {\n",
        "    'train_img':'train-images-idx3-ubyte.gz',\n",
        "    'train_label':'train-labels-idx1-ubyte.gz',\n",
        "    'test_img':'t10k-images-idx3-ubyte.gz',\n",
        "    'test_label':'t10k-labels-idx1-ubyte.gz'\n",
        "}\n",
        "\n",
        "# 해당 경로가 없을 시 디렉토리 새로 생성\n",
        "os.makedirs(dataset_dir, exist_ok=True)\n",
        "\n",
        "# 해당 경로에 존재하지 않는 파일을 모두 다운로드\n",
        "for filename in key_file.values():\n",
        "    if filename not in os.listdir(dataset_dir):\n",
        "        urlretrieve(url + filename, os.path.join(dataset_dir, filename))\n",
        "        print(\"Downloaded %s to %s\" % (filename, dataset_dir))"
      ],
      "metadata": {
        "id": "PIquK8DVUven"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 👨‍💻 <font color='green'><b>[ 코드 ]</b></font> MNIST 데이터셋 불러오기"
      ],
      "metadata": {
        "id": "se_BadrRXqwk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-08-27T16:16:01.905339Z",
          "start_time": "2023-08-27T16:16:01.891274Z"
        },
        "id": "FzT3dDC_FVnl"
      },
      "outputs": [],
      "source": [
        "def _load_label(file_name):\n",
        "    \"\"\"MNIST 데이터셋 라벨을 NumPy Array로 변환하여 불러오기\n",
        "    \"\"\"\n",
        "    file_path = dataset_dir + \"/\" + file_name\n",
        "    with gzip.open(file_path, 'rb') as f:\n",
        "        labels = np.frombuffer(f.read(), np.uint8, offset=8)\n",
        "    return labels\n",
        "\n",
        "def _load_img(file_name):\n",
        "    \"\"\"MNIST 데이터셋 이미지을 NumPy Array로 변환하여 불러오기\n",
        "    \"\"\"\n",
        "    file_path = dataset_dir + \"/\" + file_name\n",
        "    with gzip.open(file_path, 'rb') as f:\n",
        "        data = np.frombuffer(f.read(), np.uint8, offset=16)\n",
        "    data = data.reshape(-1, 28*28)\n",
        "    return data\n",
        "\n",
        "def _convert_numpy():\n",
        "    \"\"\"NumPy Array로 불러온 MNIST 데이터셋을 딕셔너리로 매핑\n",
        "    \"\"\"\n",
        "    dataset = {}\n",
        "    dataset['train_img'] =  _load_img(key_file['train_img'])\n",
        "    dataset['train_label'] = _load_label(key_file['train_label'])\n",
        "    dataset['test_img'] = _load_img(key_file['test_img'])\n",
        "    dataset['test_label'] = _load_label(key_file['test_label'])\n",
        "    return dataset\n",
        "\n",
        "def init_mnist():\n",
        "    \"\"\"MNIST 데이터셋을 Pickle화\n",
        "    \"\"\"\n",
        "    dataset = _convert_numpy()\n",
        "    with open(save_file, 'wb') as f:\n",
        "        pickle.dump(dataset, f, -1)\n",
        "\n",
        "def _change_one_hot_label(X):\n",
        "    T = np.zeros((X.size, 10))\n",
        "    for idx, row in enumerate(T):\n",
        "        row[X[idx]] = 1\n",
        "    return T\n",
        "\n",
        "def load_mnist(normalize=True, flatten=True, one_hot_label=False):\n",
        "    \"\"\"MNIST 데이터셋 읽기\n",
        "    \"\"\"\n",
        "    # Pickle화 됐는지 확인\n",
        "    if not os.path.exists(save_file):\n",
        "        init_mnist()\n",
        "\n",
        "    # Pickle화된 MNIST 데이터셋 가져오기\n",
        "    with open(save_file, 'rb') as f:\n",
        "        dataset = pickle.load(f)\n",
        "\n",
        "    # 이미지의 픽셀 값을 0.0~1.0 사이의 값으로 정규화\n",
        "    if normalize:\n",
        "        for key in ('train_img', 'test_img'):\n",
        "            dataset[key] = dataset[key].astype(np.float32)\n",
        "            dataset[key] /= 255.0\n",
        "\n",
        "    # 레이블을 원-핫(one-hot) 배열로 변환\n",
        "    if one_hot_label:\n",
        "        dataset['train_label'] = _change_one_hot_label(dataset['train_label'])\n",
        "        dataset['test_label'] = _change_one_hot_label(dataset['test_label'])\n",
        "\n",
        "    # 입력 이미지를 1차원 배열로 만듬\n",
        "    if not flatten:\n",
        "         for key in ('train_img', 'test_img'):\n",
        "            dataset[key] = dataset[key].reshape(-1, 1, 28, 28)\n",
        "\n",
        "    return (dataset['train_img'], dataset['train_label']), (dataset['test_img'], dataset['test_label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-08-27T16:16:02.039304Z",
          "start_time": "2023-08-27T16:16:01.906801Z"
        },
        "id": "Jor5XkIJFVnl"
      },
      "outputs": [],
      "source": [
        "# MNIST 데이터셋 불러오기\n",
        "(X_train, y_train), (X_test, y_test) = load_mnist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-08-27T16:16:02.090219Z",
          "start_time": "2023-08-27T16:16:02.040854Z"
        },
        "id": "II9YQIaUFVnl"
      },
      "outputs": [],
      "source": [
        "# 학습 데이터의 인덱스를 랜덤하게 셔플\n",
        "indices = np.arange(X_train.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "\n",
        "# 섞인 인덱스를 기반으로 train과 valid 데이터 분할\n",
        "# 과적합이 되는 상황을 유도하기 위해서 데이터의 수를 줄임\n",
        "valid_idx = indices[:10000]\n",
        "train_idx = indices[10000:]\n",
        "\n",
        "# 학습 데이터 중 일부를 검증 데이터로 활용\n",
        "X_valid, y_valid = X_train[valid_idx], y_train[valid_idx]\n",
        "X_train, y_train = X_train[train_idx], y_train[train_idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-08-27T16:16:02.094224Z",
          "start_time": "2023-08-27T16:16:02.091652Z"
        },
        "id": "EKUw0Hb1FVnl"
      },
      "outputs": [],
      "source": [
        "# MNIST 데이터셋 살펴보기\n",
        "print(f\"학습 데이터: {len(X_train):,}개\\n검증 데이터: {len(X_valid):,}개\\n평가 데이터: {len(X_test):,}개\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 👨‍💻 <font color='green'><b>[ 코드 ]</b></font> MNIST 데이터셋 살펴보기"
      ],
      "metadata": {
        "id": "eg0uTq-2XusH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-08-27T16:16:02.917201Z",
          "start_time": "2023-08-27T16:16:02.096374Z"
        },
        "id": "sZ5OI33TFVnl"
      },
      "outputs": [],
      "source": [
        "# MNIST 데이터셋 중 임의로 16개의 샘플 살펴봅시다\n",
        "plt.figure(figsize=(7,7))\n",
        "for n, i in enumerate(np.random.randint(0, len(X_train), size=16), start=1):\n",
        "    plt.subplot(4,4,n)\n",
        "    plt.imshow(X_train[i].reshape(28,28), cmap='gray')\n",
        "    plt.title(f\"Label: {y_train[i]}\", fontsize=15)\n",
        "    plt.axis('off')\n",
        "plt.suptitle('MNIST Dataset', fontsize=20)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_F7rHjuOFVnm"
      },
      "source": [
        "### Trainer 클래스를 활용하여 딥러닝 파이프라인 만들기\n",
        "\n",
        "이번 실습에서는 Trainer 객체를 중심으로 간단한 신경망 모델을 구현해볼 예정입니다. Trainer 객체는 모델 학습의 전체 과정을 효율적으로 관리하고, 코드의 재사용성을 높이는 역할을 합니다. 이를 위해 각각의 활성화 레이어, 완전 연결 레이어, 소프트맥스 레이어를 구현한 뒤, 옵티마이저와 평가지표를 설정합니다. 이러한 구성요소들을 모아 전체적인 신경망을 만들고, 최종적으로는 이를 관리할 Trainer 객체를 구현합니다. 아래는 성능을 높이기 위한 기본 구현 코드입니다.\n",
        "\n",
        "- 👨‍💻 <font color='green'><b>[ 코드 ]</b></font> 1. 시그모이드 활성화 레이어 구현하기\n",
        "- 👨‍💻 <font color='green'><b>[ 코드 ]</b></font> 2. ReLU 활성화 레이어 구현하기\n",
        "- 👨‍💻 <font color='green'><b>[ 코드 ]</b></font> 3. 완전 연결 레이어 구현하기\n",
        "- 👨‍💻 <font color='green'><b>[ 코드 ]</b></font> 4. 소프트맥스 레이어 구현하기\n",
        "- 👨‍💻 <font color='green'><b>[ 코드 ]</b></font> 5. 전체적인 신경망 구현하기\n",
        "- 👨‍💻 <font color='green'><b>[ 코드 ]</b></font> 6. 옵티마이저 구현하기\n",
        "- 👨‍💻 <font color='green'><b>[ 코드 ]</b></font> 7. 평가 지표 구현하기\n",
        "- 👨‍💻 <font color='green'><b>[ 코드 ]</b></font> 8. Trainer 객체 구현하기\n",
        "\n",
        "이렇게 Trainer 객체를 중심으로 구현하는 방식은 코드의 구조를 더 명확하게 하고, 나중에 다른 프로젝트나 더 복잡한 모델로 확장하기 쉽게 만들어줍니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gY08LuNIFVnm"
      },
      "source": [
        "#### 👨‍💻 <font color='green'><b>[ 코드 ]</b></font> 1. 시그모이드 활성화 레이어 구현하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-08-27T16:16:02.921825Z",
          "start_time": "2023-08-27T16:16:02.919146Z"
        },
        "id": "P-l1FSXgFVnm"
      },
      "outputs": [],
      "source": [
        "def sigmoid(x):\n",
        "    '''시그모이드 함수'''\n",
        "    return 1 / (1 + np.exp(-x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-08-27T16:16:02.936400Z",
          "start_time": "2023-08-27T16:16:02.923217Z"
        },
        "id": "ywFinkhhFVnm"
      },
      "outputs": [],
      "source": [
        "def sigmoid_grad(x):\n",
        "    '''시그모이드 함수의 그래디언트'''\n",
        "    return (1.0 - sigmoid(x)) * sigmoid(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-08-27T16:16:02.950176Z",
          "start_time": "2023-08-27T16:16:02.937770Z"
        },
        "id": "J5onIEBoFVnm"
      },
      "outputs": [],
      "source": [
        "class Sigmoid:\n",
        "    '''시그모이드 레이어'''\n",
        "    def __init__(self):\n",
        "        self.out = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        '''시그모이드 레이어의 순전파(forward propagation)'''\n",
        "        self.out = sigmoid(x)  # 순전파가 흐를 시 그 결과물을 attribute로 저장 및 리턴\n",
        "        return self.out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        '''시그모이드 레이어의 역전파(backpropagation)'''\n",
        "        dx = dout * (1.0 - self.out) * self.out\n",
        "        return dx  # 역전파가 흐를시 그 그래디언트 값을 리턴"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDiesTHZFVnm"
      },
      "source": [
        "#### 👨‍💻 <font color='green'><b>[ 코드 ]</b></font> 2. ReLU 활성화 레이어 구현하기 (NEW!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-08-27T16:16:02.967351Z",
          "start_time": "2023-08-27T16:16:02.951575Z"
        },
        "id": "2ZtNajsaFVnm"
      },
      "outputs": [],
      "source": [
        "def relu(x):\n",
        "    '''ReLU 함수'''\n",
        "    return np.maximum(0, x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-08-27T16:16:02.981659Z",
          "start_time": "2023-08-27T16:16:02.968928Z"
        },
        "id": "lRH_u6DiFVnm"
      },
      "outputs": [],
      "source": [
        "def relu_grad(x):\n",
        "    '''ReLU 함수의 그래디언트'''\n",
        "    grad = np.zeros(x)\n",
        "    grad[x >= 0] = 1\n",
        "    return grad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-08-27T16:16:02.995808Z",
          "start_time": "2023-08-27T16:16:02.983090Z"
        },
        "id": "k72dcmbtFVnm"
      },
      "outputs": [],
      "source": [
        "class Relu:\n",
        "    '''ReLU 레이어'''\n",
        "    def __init__(self):\n",
        "        self.mask = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        '''ReLU 레이어의 순전파(forward propagation)'''\n",
        "        self.mask = (x <= 0)\n",
        "        out = x.copy()\n",
        "        out[self.mask] = 0\n",
        "        return out  # 순전파가 흐를 시 그 결과물을 attribute로 저장 및 리턴\n",
        "\n",
        "    def backward(self, dout):\n",
        "        '''ReLU 레이어의 역전파(backpropagation)'''\n",
        "        dout[self.mask] = 0\n",
        "        dx = dout\n",
        "        return dx  # 역전파가 흐를시 그 그래디언트 값을 리턴"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uO2J8OEhFVnm"
      },
      "source": [
        "#### 👨‍💻 <font color='green'><b>[ 코드 ]</b></font> 3. 완전 연결 레이어 구현하기\n",
        "\n",
        "> 신경망을 구성하게 될 각각의 MLP 레이어를 클래스 형태로 구현합니다.\n",
        "> <br>MLP 모델을 구성하는 레이어는 fully-connected layer, linear layer, dense layer 등의 다양한 이름으로 불리고 있습니다.\n",
        "> <br>본 실습에서는 `FCLayer`라는 이름으로 클래스를 정의하도록 하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-08-27T16:16:03.036884Z",
          "start_time": "2023-08-27T16:16:02.997214Z"
        },
        "id": "IVnQJFclFVnm"
      },
      "outputs": [],
      "source": [
        "class FCLayer:\n",
        "    '''완전 연결 레이어'''\n",
        "    def __init__(self, W, b):\n",
        "        self.W = W\n",
        "        self.b = b\n",
        "        self.x = None\n",
        "        self.original_x_shape = None\n",
        "        self.dW = None\n",
        "        self.db = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        '''완전 연결 레이어의 순전파(forward propagation)'''\n",
        "        self.original_x_shape = x.shape\n",
        "        x = x.reshape(x.shape[0], -1)\n",
        "        self.x = x\n",
        "        out = np.dot(self.x, self.W) + self.b\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        '''완전 연결 레이어의 역전파(backpropagation)'''\n",
        "        dx = np.dot(dout, self.W.T)\n",
        "        self.dW = np.dot(self.x.T, dout)\n",
        "        self.db = np.sum(dout, axis=0)\n",
        "        dx = dx.reshape(*self.original_x_shape)\n",
        "\n",
        "        return dx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apCOzZT8FVnm"
      },
      "source": [
        "#### 👨‍💻 <font color='green'><b>[ 코드 ]</b></font> 4. 소프트맥스 레이어 구현하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-08-27T16:16:03.050827Z",
          "start_time": "2023-08-27T16:16:03.038365Z"
        },
        "id": "4-0I9BU3FVnm"
      },
      "outputs": [],
      "source": [
        "def softmax(x):\n",
        "    '''소프트맥스 함수'''\n",
        "    if x.ndim == 2:\n",
        "        x = x.T\n",
        "        x = x - np.max(x, axis=0)\n",
        "        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
        "        return y.T\n",
        "\n",
        "    x = x - np.max(x)\n",
        "\n",
        "    return np.exp(x) / np.sum(np.exp(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 📝 <font color='orange'><b>[ 설명 ]</b></font> 교차엔트로피(CE) 손실 함수 및 도함수의 수식\n",
        "\n",
        "$$\n",
        "L(y,\\hat{y}) = \\frac{-1}{n}\\sum_{i=1}^n y_i\\log(\\hat{y}_i)+(1-y_i)(\\log(1-\\hat{y}_i)\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\frac{dL}{d\\hat y} = \\frac{-1}{n}\\left(\\frac{y_i}{\\hat{y}_i} - \\frac{1-y_i}{1 - \\hat{y}_i}\\right) = \\frac{\\hat{y}_i-y_i}{n(1-\\hat{y}_i)\\hat{y}_i}\n",
        "$$"
      ],
      "metadata": {
        "id": "2jrNlwnnTBiu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-08-27T16:16:03.068765Z",
          "start_time": "2023-08-27T16:16:03.052397Z"
        },
        "id": "wUCPtQVyFVnm"
      },
      "outputs": [],
      "source": [
        "def cross_entropy_error(y_true, y_pred):\n",
        "    if y_true.ndim == 1:\n",
        "        y_pred = y_pred.reshape(1, y_pred.size)\n",
        "        y_true = y_true.reshape(1, y_true.size)\n",
        "\n",
        "    if y_pred.size == y_true.size:\n",
        "        y_pred = y_pred.argmax(axis=1)\n",
        "\n",
        "    batch_size = y_true.shape[0]\n",
        "\n",
        "    return -np.sum(np.log(y_true[np.arange(batch_size), y_pred] + 1e-7)) / batch_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-08-27T16:16:03.083182Z",
          "start_time": "2023-08-27T16:16:03.070116Z"
        },
        "id": "kLq5qtoRFVnm"
      },
      "outputs": [],
      "source": [
        "class Softmax:\n",
        "    '''소프트맥스 레이어'''\n",
        "    def __init__(self):\n",
        "        self.loss = None\n",
        "        self.y_true = None\n",
        "        self.y_pred = None\n",
        "\n",
        "    def forward(self, x, y_pred):\n",
        "        '''소프트맥스 레이어의 순전파(forward propagation)'''\n",
        "        self.y_true = softmax(x)\n",
        "        self.y_pred = y_pred\n",
        "        self.loss = cross_entropy_error(self.y_true, self.y_pred)\n",
        "\n",
        "        return self.loss\n",
        "\n",
        "    def backward(self, dout=1):\n",
        "        '''소프트맥스 레이어의 역전파(backpropagation)'''\n",
        "        batch_size = self.y_pred.shape[0]\n",
        "        if self.y_pred.size == self.y_true.size:\n",
        "            dx = (self.y_true - self.y_pred) / batch_size\n",
        "        else:\n",
        "            dx = self.y_true.copy()\n",
        "            dx[np.arange(batch_size), self.y_pred] -= 1\n",
        "            dx = dx / batch_size\n",
        "\n",
        "        return dx"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 👨‍💻 <font color='green'><b>[ 코드 ]</b></font> 5. 전체적인 신경망 구현하기\n",
        "앞서 구현한 모든 레이어를 바탕으로 최종적인 신경망을 구현해볼까요?"
      ],
      "metadata": {
        "id": "G3yYbwjjPhTE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-08-27T16:16:03.098681Z",
          "start_time": "2023-08-27T16:16:03.084626Z"
        },
        "id": "vZN4wGmUFVnm"
      },
      "outputs": [],
      "source": [
        "class Net:\n",
        "    def __init__(self,\n",
        "                 input_size,\n",
        "                 hidden_size_list,\n",
        "                 output_size,\n",
        "                 use_dropout=False,\n",
        "                 dropout_ratio=0,\n",
        "                 use_batchnorm=False,\n",
        "                 activation='relu',\n",
        "                 weight_init_std='relu',\n",
        "                 weight_decay_lambda=0\n",
        "                 ):\n",
        "\n",
        "        # 네트워크의 초기화\n",
        "        self.input_size = input_size                    # 입력 크기 (예: 이미지의 픽셀 수)\n",
        "        self.output_size = output_size                  # 출력 크기 (예: 분류할 클래스 수)\n",
        "        self.hidden_size_list = hidden_size_list        # 은닉층의 뉴런 수 리스트\n",
        "        self.hidden_layer_num = len(hidden_size_list)   # 은닉층의 개수\n",
        "        self.weight_decay_lambda = weight_decay_lambda  # 가중치 감쇠\n",
        "        self.use_dropout = use_dropout                  # 드롭아웃 사용 여부\n",
        "        self.dropout_ratio = dropout_ratio              # 드롭아웃 사용시 계수\n",
        "        self.use_batchnorm = use_batchnorm              # 배치 정규화 사용 여부\n",
        "\n",
        "        # 신경망의 가중치\n",
        "        self.params = {}\n",
        "\n",
        "        # 가중치 초기화\n",
        "        self.__init_weight(weight_init_std)\n",
        "\n",
        "        # 활성화 함수 지정 (ReLU, Sigmoid 등)\n",
        "        activation_layer = {'sigmoid': Sigmoid, 'relu': Relu}\n",
        "\n",
        "        # 신경망의 레이어\n",
        "        self.layers = OrderedDict()\n",
        "\n",
        "        # 은닉층 생성\n",
        "        for idx in range(1, self.hidden_layer_num+1):\n",
        "\n",
        "            # 완전 연결 레이어 (Fully Connected Layer)\n",
        "            self.layers[f'FC{idx}'] = FCLayer(\n",
        "                self.params[f'W{idx}'],\n",
        "                self.params[f'b{idx}']\n",
        "            )\n",
        "\n",
        "            # 배치 정규화 사용 여부에 따른 레이어 추가\n",
        "            if self.use_batchnorm:\n",
        "                self.params[f'gamma{idx}'] = np.ones(hidden_size_list[idx-1])\n",
        "                self.params[f'beta{idx}'] = np.zeros(hidden_size_list[idx-1])\n",
        "                self.layers[f'BN{idx}'] = BatchNormalization(\n",
        "                    self.params[f'gamma{idx}'],\n",
        "                    self.params[f'beta{idx}']\n",
        "                )\n",
        "\n",
        "            # 활성화 레이어 (Activation Layer: 예를 들면 ReLU나 Sigmoid)\n",
        "            self.layers[f'Act{idx}'] = activation_layer[activation]()\n",
        "\n",
        "            # 드롭아웃 사용 여부에 따른 레이어 추가\n",
        "            if self.use_dropout:\n",
        "                self.layers[f'Dropout{idx}'] = Dropout(self.dropout_ratio)\n",
        "\n",
        "        # 출력층 (Last layer)\n",
        "        idx = self.hidden_layer_num + 1\n",
        "        self.layers[f'FC{idx}'] = FCLayer(\n",
        "            self.params[f'W{idx}'],\n",
        "            self.params[f'b{idx}']\n",
        "        )\n",
        "\n",
        "        # 소프트맥스 함수를 마지막 계층으로 설정 (분류 문제의 경우)\n",
        "        self.last_layer = Softmax()\n",
        "\n",
        "    def __init_weight(self, weight_init_std):\n",
        "        \"\"\"\n",
        "        신경망의 가중치를 초기화합니다.\n",
        "        Xavier 방식과 He 방식이 있습니다.\n",
        "        \"\"\"\n",
        "        # 전체 네트워크의 각 층의 뉴런 수를 리스트로 구성\n",
        "        all_size_list = [self.input_size] + \\\n",
        "            self.hidden_size_list + [self.output_size]\n",
        "\n",
        "        # 모든 층을 순회하며 가중치를 초기화\n",
        "        for idx in range(1, len(all_size_list)):\n",
        "            scale = weight_init_std\n",
        "\n",
        "            # ReLU 활성화 함수를 사용할 경우 He 초기화 방법 사용\n",
        "            if str(weight_init_std).lower() in ('relu', 'he'):\n",
        "                scale = np.sqrt(2.0 / all_size_list[idx - 1])\n",
        "\n",
        "            # Sigmoid 활성화 함수를 사용할 경우 Xavier 초기화 방법 사용\n",
        "            elif str(weight_init_std).lower() in ('sigmoid', 'xavier'):\n",
        "                scale = np.sqrt(1.0 / all_size_list[idx - 1])\n",
        "\n",
        "            # 가중치와 편향 초기화\n",
        "            self.params[f'W{idx}'] = scale * \\\n",
        "                np.random.randn(all_size_list[idx-1], all_size_list[idx])\n",
        "            self.params[f'b{idx}'] = np.zeros(all_size_list[idx])\n",
        "\n",
        "    def predict(self, x, train_flag=False):\n",
        "        \"\"\"\n",
        "        입력 데이터(x)에 대한 예측 값을 반환합니다.\n",
        "        \"\"\"\n",
        "        for key, layer in self.layers.items():\n",
        "            # 드롭아웃 또는 배치 정규화 레이어일 경우 train_flag를 사용\n",
        "            if \"Dropout\" in key or \"BatchNorm\" in key:\n",
        "                x = layer.forward(x, train_flag)\n",
        "            else:\n",
        "                x = layer.forward(x)\n",
        "        return x\n",
        "\n",
        "    def loss(self, y_pred, y_true):\n",
        "        \"\"\"\n",
        "        정답(y_true)와 예측 값(y_pred)을 기반으로 손실 값을 계산합니다.\n",
        "        L2 정규화(L2 Regularization)가 적용된 가중치 감쇠를 포함합니다.\n",
        "        \"\"\"\n",
        "        weight_decay = 0\n",
        "\n",
        "        # 가중치 감쇠 계산\n",
        "        for idx in range(1, self.hidden_layer_num + 2):\n",
        "            W = self.params[f'W{idx}']\n",
        "            weight_decay += 0.5 * self.weight_decay_lambda * np.sum(W ** 2)\n",
        "\n",
        "        return self.last_layer.forward(y_true, y_pred) + weight_decay\n",
        "\n",
        "    def gradient(self):\n",
        "        \"\"\"\n",
        "        신경망의 그래디언트(미분값)를 계산합니다.\n",
        "        이는 네트워크의 파라미터 업데이트에 사용됩니다.\n",
        "        \"\"\"\n",
        "        # 역전파 시작\n",
        "        dout = 1\n",
        "        dout = self.last_layer.backward(dout)\n",
        "\n",
        "        layers = list(self.layers.values())\n",
        "        layers.reverse()\n",
        "        for layer in layers:\n",
        "            dout = layer.backward(dout)\n",
        "\n",
        "        # 그래디언트 초기화\n",
        "        grads = {}\n",
        "        for idx in range(1, self.hidden_layer_num+2):\n",
        "            grads[f'W{idx}'] = self.layers[f'FC{idx}'].dW + \\\n",
        "                self.weight_decay_lambda * self.layers[f'FC{idx}'].W\n",
        "            grads[f'b{idx}'] = self.layers[f'FC{idx}'].db\n",
        "\n",
        "            # 배치 정규화를 사용하는 경우 gamma와 beta에 대한 그래디언트도 계산\n",
        "            if self.use_batchnorm and idx != self.hidden_layer_num+1:\n",
        "                grads[f'gamma{idx}'] = self.layers[f'BN{idx}'].dgamma\n",
        "                grads[f'beta{idx}'] = self.layers[f'BN{idx}'].dbeta\n",
        "\n",
        "        return grads"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 👨‍💻 <font color='green'><b>[ 코드 ]</b></font> 6. 옵티마이저 구현하기  (NEW!)\n",
        "SGD 옵티마이저를 클래스로 만들어 update함수를 구현합니다."
      ],
      "metadata": {
        "id": "LNiY4P9ZUHSE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-08-27T16:16:03.134882Z",
          "start_time": "2023-08-27T16:16:03.118571Z"
        },
        "id": "AcTh3kaTFVnn"
      },
      "outputs": [],
      "source": [
        "class SGD:\n",
        "    '''SGD 옵티마이저'''\n",
        "    def __init__(self, lr=0.01):\n",
        "        self.lr = lr\n",
        "\n",
        "    def update(self, params, grads):\n",
        "        for key in params.keys():\n",
        "            params[key] -= self.lr * grads[key]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 👨‍💻 <font color='green'><b>[ 코드 ]</b></font> 7. 평가 지표 구현하기  (NEW!)\n",
        "모델 평가를 위한 지표인 정확도를 함수로 구현합니다."
      ],
      "metadata": {
        "id": "H93ij8plUSEc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-08-27T16:16:03.116978Z",
          "start_time": "2023-08-27T16:16:03.100073Z"
        },
        "id": "uyMd1ut1FVnm"
      },
      "outputs": [],
      "source": [
        "def accuracy(y_true, y_pred, batch_size):\n",
        "    \"\"\"\n",
        "    정확도를 계산합니다.\n",
        "    \"\"\"\n",
        "    y_pred = np.argmax(y_pred, axis=1)\n",
        "\n",
        "    if y_pred.ndim != 1:\n",
        "        y_pred = np.argmax(y_pred, axis=1)\n",
        "\n",
        "    assert y_true.shape == y_pred.shape\n",
        "\n",
        "    # 실제 라벨과 예측 라벨이 일치하는 개수를 계산하여 정확도 반환\n",
        "    return np.sum(y_true == y_pred) / float(batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 👨‍💻 <font color='green'><b>[ 코드 ]</b></font> 8. Trainer 구현하기  (NEW!)\n",
        "\n",
        "위에서 준비한 신경망과 옵티마이저를 이용해서 학습할 수 있게 Trainer 클래스를 구현합니다. 이는 이후 이어지는 실험들을 효율적으로 할 수 있게 도와줍니다."
      ],
      "metadata": {
        "id": "EnSv1rZHT5HD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-08-27T16:16:03.154181Z",
          "start_time": "2023-08-27T16:16:03.136327Z"
        },
        "id": "DFol4S6eFVnn"
      },
      "outputs": [],
      "source": [
        "class Trainer:\n",
        "    def __init__(\n",
        "        self,\n",
        "        X_train,\n",
        "        y_train,\n",
        "        X_valid,\n",
        "        y_valid,\n",
        "        model,\n",
        "        optimizer,\n",
        "        max_iterations,\n",
        "        batch_size,\n",
        "    ):\n",
        "\n",
        "        # 데이터셋\n",
        "        self.X_train = X_train\n",
        "        self.y_train = y_train\n",
        "        self.X_valid = X_valid\n",
        "        self.y_valid = y_valid\n",
        "\n",
        "        # 모델\n",
        "        self.model = model\n",
        "\n",
        "        # 옵티마이저\n",
        "        self.optimizer = optimizer\n",
        "\n",
        "        self.train_size = X_train.shape[0]\n",
        "        self.batch_size = batch_size\n",
        "        self.max_iterations = max_iterations\n",
        "\n",
        "        self.train_loss_list = []\n",
        "        self.train_acc_list = []\n",
        "        self.valid_loss_list = []\n",
        "        self.valid_acc_list = []\n",
        "\n",
        "    def run(self,):\n",
        "        \"\"\"신경망과 옵티마이저를 이용해서 학습하는 함수\"\"\"\n",
        "        start_time = datetime.datetime.now()\n",
        "\n",
        "        for i in range(self.max_iterations):\n",
        "            ####################################################################\n",
        "            # 학습\n",
        "            ####################################################################\n",
        "            batch_mask = np.random.choice(self.train_size, self.batch_size)\n",
        "            X_batch = self.X_train[batch_mask]\n",
        "            y_batch = self.y_train[batch_mask]\n",
        "\n",
        "            # 예측 수행\n",
        "            y_pred = self.model.predict(X_batch, train_flag=True)\n",
        "\n",
        "            # 손실 계산\n",
        "            train_loss = self.model.loss(y_batch, y_pred)\n",
        "            self.train_loss_list.append(train_loss)\n",
        "\n",
        "            # 정확도 계산\n",
        "            train_acc = accuracy(y_batch, y_pred, self.batch_size)\n",
        "            self.train_acc_list.append(train_acc)\n",
        "\n",
        "            # Gradient 계산\n",
        "            grads = self.model.gradient()\n",
        "\n",
        "            # Optimizer 업데이트\n",
        "            self.optimizer.update(self.model.params, grads)\n",
        "\n",
        "            ####################################################################\n",
        "            # 검증\n",
        "            ####################################################################\n",
        "            batch_mask = np.random.choice(X_valid.shape[0], self.batch_size)\n",
        "            X_batch = self.X_valid[batch_mask]\n",
        "            y_batch = self.y_valid[batch_mask]\n",
        "\n",
        "            # 예측 수행\n",
        "            y_pred = self.model.predict(X_batch, train_flag=False)\n",
        "\n",
        "            # 손실 계산\n",
        "            valid_loss = self.model.loss(y_batch, y_pred)\n",
        "            self.valid_loss_list.append(valid_loss)\n",
        "\n",
        "            # 정확도 계산\n",
        "            valid_acc = accuracy(y_batch, y_pred, self.batch_size)\n",
        "            self.valid_acc_list.append(valid_acc)\n",
        "\n",
        "            if i % 100 == 0:\n",
        "                elpased_time = datetime.datetime.now() - start_time\n",
        "                msg = f\"\\033[31m[Elpased Time: {elpased_time}]\\033[0m \"\n",
        "                msg += f\"Iter: {i:>4} \"\n",
        "                msg += f\"Train Loss : {train_loss:.4f} \"\n",
        "                msg += f\"Train Acc : {train_acc:.2f} \"\n",
        "                msg += f\"Valid Loss : {valid_loss:.4f} \"\n",
        "                msg += f\"Valid Acc : {valid_acc:.2f} \"\n",
        "                print(msg)\n",
        "\n",
        "    def show_results(self,):\n",
        "        \"\"\"학습된 결과를 시각화해주는 함수\"\"\"\n",
        "        plt.figure(figsize=(16,8))\n",
        "\n",
        "        plt.subplot(2,1,1)\n",
        "        plt.plot(self.train_loss_list)\n",
        "        plt.plot(self.valid_loss_list)\n",
        "        plt.xlabel('Iterations')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.legend(['Train', 'Valid'])\n",
        "        plt.grid(True)\n",
        "\n",
        "        plt.subplot(2,1,2)\n",
        "        plt.plot(self.train_acc_list)\n",
        "        plt.plot(self.valid_acc_list)\n",
        "        plt.xlabel('Iterations')\n",
        "        plt.ylabel('Accuracy')\n",
        "        plt.legend(['Train', 'Valid'])\n",
        "        plt.ylim(0,1.05)\n",
        "        plt.grid(True)\n",
        "\n",
        "        plt.suptitle(f'Result - Loss & Acc', fontsize=20)\n",
        "        plt.tight_layout()\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 📝 <font color='orange'><b>[ 설명 ]</b></font> Trainer 예제\n",
        "Trainer 객체를 생성해서 여기에 신경망과 옵티마이저를 넣어 실행시켜봅시다. 실행시킬 때는 `run`메소드를 이용합니다."
      ],
      "metadata": {
        "id": "gSHsI07sUjJZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-08-27T16:16:17.269204Z",
          "start_time": "2023-08-27T16:16:03.194936Z"
        },
        "id": "nmtzSK_oFVnn"
      },
      "outputs": [],
      "source": [
        "# 모델\n",
        "model = Net(\n",
        "    input_size=784,\n",
        "    hidden_size_list=[100, 100, 100],\n",
        "    output_size=10,\n",
        "    use_dropout=False,\n",
        "    use_batchnorm=False,\n",
        "    weight_decay_lambda=0,\n",
        ")\n",
        "\n",
        "# 옵티마이저\n",
        "optimizer = SGD(lr=0.01)\n",
        "\n",
        "# 모델 학습 및 검증 시작\n",
        "trainer_base = Trainer(\n",
        "    X_train[:200], y_train[:200], # 과적합을 발생시키기 위해 모델에 비해 데이터셋이 부족하게 설정\n",
        "    X_valid, y_valid,\n",
        "    model=model,\n",
        "    optimizer=optimizer,\n",
        "    max_iterations=2000,\n",
        "    batch_size=128,\n",
        ")\n",
        "trainer_base.run()\n",
        "\n",
        "# 결과 시각화\n",
        "trainer_base.show_results()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9Ua2kXdFVnn"
      },
      "source": [
        "## 1. 드롭아웃 (Dropout)\n",
        "\n",
        "<hr style=\"height:5px;border:none;color:#5F71F7;background-color:#5F71F7\">\n",
        "\n",
        "```\n",
        "💡 목차 개요 : 신경망의 과적합을 방지하기 위한 방법 중 하나인 드롭아웃에 대해서 알아보고, 이를 넘파이를 통해 직접 구현해보는 시간을 가집니다.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kzrx4J1SFVnn"
      },
      "source": [
        "#### 👨‍💻 <font color='green'><b>[ 코드 ]</b></font> 드롭아웃 구현하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-08-27T16:16:17.274621Z",
          "start_time": "2023-08-27T16:16:17.270648Z"
        },
        "id": "hNo1l4isFVnn"
      },
      "outputs": [],
      "source": [
        "# 신경망 학습 중 과적합을 방지하기 위한 Dropout 기법을 구현합니다.\n",
        "class Dropout:\n",
        "\n",
        "    # 초기화 함수\n",
        "    def __init__(self, dropout_ratio=0.5):\n",
        "\n",
        "        # dropout_ratio: 드롭아웃할 뉴런의 비율\n",
        "        # e.g., 0.5는 50%의 뉴런을 무작위로 꺼버림\n",
        "        self.dropout_ratio = dropout_ratio\n",
        "\n",
        "        # mask: 드롭아웃할 뉴런을 결정하는 불리언 마스크.\n",
        "        # 이는 오직 학습 시에만 사용됨.\n",
        "        self.mask = None\n",
        "\n",
        "    # 순전파 함수\n",
        "    def forward(self, x, train_flag=True):\n",
        "\n",
        "        # 학습 시 (train_flag가 True일 때)\n",
        "        if train_flag:\n",
        "\n",
        "            # 입력 데이터 x와 동일한 모양의 무작위 배열을 생성하고,\n",
        "            # dropout_ratio보다 큰 값만 True로 설정\n",
        "            # 이로써 어떤 뉴런을 꺼버릴지 (False)와 그대로 둘지 (True) 결정됨.\n",
        "            self.mask = np.random.rand(*x.shape) > self.dropout_ratio\n",
        "\n",
        "            # mask를 사용해 x의 일부 뉴런을 꺼버림.\n",
        "            return x * self.mask\n",
        "\n",
        "        # 테스트 시 (train_flag가 False일 때)\n",
        "        else:\n",
        "\n",
        "            # Dropout의 비율만큼 스케일 조정하여 출력\n",
        "            return x * (1.0 - self.dropout_ratio)\n",
        "\n",
        "    # 역전파 함수\n",
        "    def backward(self, dout):\n",
        "\n",
        "        # mask를 사용하여, 순전파 때 꺼진 뉴런은 그래디언트도 전달되지 않게 함.\n",
        "        return dout * self.mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8uNJKeBLFVnn"
      },
      "source": [
        "#### 📝 <font color='orange'><b>[ 설명 ]</b></font>\n",
        "* 학습 시 일부 뉴런을 무작위로 꺼버리는 방식으로 동작합니다. 이로 인해 모델이 특정 뉴런에만 의존하지 않게 되어 과적합을 방지할 수 있습니다.\n",
        "* 드롭아웃 기법은 학습 중 일부 뉴런을 무작위로 '꺼버려서' 모델이 특정 뉴런들에 지나치게 의존하는 것을 방지합니다. 이를 통해 과적합을 방지할 수 있습니다.\n",
        "* 마스크는 학습 시 사용되는 불리언 배열로, 어떤 뉴런을 꺼버릴지 결정합니다. True인 뉴런은 활성화 상태를 유지하고, False인 뉴런은 꺼버립니다.\n",
        "* 테스트 시에는 모든 뉴런을 사용하지만, 출력을 스케일링하여 학습 때 끄게 되는 뉴런의 비율을 반영합니다. 이는 학습 때와 테스트 때의 활성 뉴런 수가 동일하도록 해줍니다.\n",
        "* 역전파 시에는, 순전파 때 꺼진 뉴런에는 그래디언트가 전달되지 않게 됩니다."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 👨‍💻 <font color='green'><b>[ 코드 ]</b></font> 위에서 구현한 드롭아웃으로 학습하기"
      ],
      "metadata": {
        "id": "KuMgv9z7VD3O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-08-27T16:16:34.784926Z",
          "start_time": "2023-08-27T16:16:17.276131Z"
        },
        "id": "1ReXgmeqFVnn"
      },
      "outputs": [],
      "source": [
        "# 모델\n",
        "model = Net(\n",
        "    input_size=784,\n",
        "    hidden_size_list=[100, 100, 100],\n",
        "    output_size=10,\n",
        "    use_dropout=True,  # 드롭아웃을 사용해봅시다.\n",
        "    dropout_ratio=0.5, # 비율을 다양하게 바꿔 실험해봅시다.\n",
        ")\n",
        "\n",
        "# 옵티마이저\n",
        "optimizer = SGD(lr=0.01)\n",
        "\n",
        "# 모델 학습 및 검증 시작\n",
        "trainer_dropout = Trainer(\n",
        "    X_train[:200], y_train[:200], # 과적합을 발생시키기 위해 모델에 비해 데이터셋이 부족하게 설정\n",
        "    X_valid, y_valid,\n",
        "    model=model,\n",
        "    optimizer=optimizer,\n",
        "    max_iterations=2000,\n",
        "    batch_size=128,\n",
        ")\n",
        "trainer_dropout.run()\n",
        "\n",
        "# 결과 시각화\n",
        "trainer_dropout.show_results()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRysYd98FVno"
      },
      "source": [
        "#### 📚 <font color='blue'><b>[ 자료 ]</b></font>\n",
        "\n",
        "*  [Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., & Salakhutdinov, R. (2014). Dropout: A Simple Way to Prevent Neural Networks from Overfitting. Journal of Machine Learning Research, 15(1), 1929-1958.](http://arxiv.org/abs/1207.0580) : 드롭아웃이 처음 소개되었던 논문입니다. 한 번 살펴보세요."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1BXYRPkFVno"
      },
      "source": [
        "## 2. 가중치 초기화 (Weight Initialization)\n",
        "\n",
        "<hr style=\"height:5px;border:none;color:#5F71F7;background-color:#5F71F7\">\n",
        "\n",
        "```\n",
        "💡 목차 개요 : 신경망의 과적합을 방지하기 위한 방법 중 하나인 가중치 초기화에 대해서 알아보고, 이를 넘파이를 통해 직접 구현해보는 시간을 가집니다.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WvaP7f7GFVno"
      },
      "source": [
        "#### 👨‍💻 <font color='green'><b>[ 코드 ]</b></font> 신경망에서 가중치 초기화 파트 살펴보기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3fEJmoOFVno"
      },
      "source": [
        "```python\n",
        "class Net:\n",
        "    def __init__(self,\n",
        "                 \n",
        "    # (중략)\n",
        "                 \n",
        "    def __init_weight(self, weight_init_std):\n",
        "        # 전체 네트워크의 각 층의 뉴런 수를 리스트로 구성\n",
        "        all_size_list = [self.input_size] + self.hidden_size_list + [self.output_size]\n",
        "\n",
        "        # 모든 층을 순회하며 가중치를 초기화\n",
        "        for idx in range(1, len(all_size_list)):\n",
        "            scale = weight_init_std\n",
        "\n",
        "            # ReLU 활성화 함수를 사용할 경우 He 초기화 방법 사용\n",
        "            if str(weight_init_std).lower() in ('relu', 'he'):\n",
        "                scale = np.sqrt(2.0 / all_size_list[idx - 1])\n",
        "\n",
        "            # Sigmoid 활성화 함수를 사용할 경우 Xavier 초기화 방법 사용\n",
        "            elif str(weight_init_std).lower() in ('sigmoid', 'xavier'):\n",
        "                scale = np.sqrt(1.0 / all_size_list[idx - 1])\n",
        "\n",
        "            # 가중치와 편향 초기화\n",
        "            self.params[f'W{idx}'] = scale * np.random.randn(all_size_list[idx-1], all_size_list[idx])\n",
        "            self.params[f'b{idx}'] = np.zeros(all_size_list[idx])\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kv6e9pYIFVno"
      },
      "source": [
        "#### 📝 <font color='orange'><b>[ 설명 ]</b></font> 가중치 초기화\n",
        "*  가중치 초기화는 딥 러닝에서 매우 중요합니다. 잘못된 초기화는 학습 속도를 느리게 하거나, 학습이 전혀 이루어지지 않게 할 수 있습니다.\n",
        "* Xavier 초기화는 활성화 함수로 Sigmoid나 Hyperbolic Tangent를 사용할 때 권장됩니다. 입력 뉴런의 수로 스케일링되어 가중치를 초기화합니다.\n",
        "* He 초기화는 ReLU와 그 변형을 활성화 함수로 사용할 때 권장됩니다. 입력 뉴런의 수의 절반으로 스케일링되어 가중치를 초기화합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JX5Yd8xzFVno"
      },
      "source": [
        "#### 👨‍💻 <font color='green'><b>[ 코드 ]</b></font> Xavier 초기화 방법으로 학습하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-08-27T16:16:50.644810Z",
          "start_time": "2023-08-27T16:16:34.786303Z"
        },
        "id": "BmZWKhYgFVno"
      },
      "outputs": [],
      "source": [
        "# 모델\n",
        "model = Net(\n",
        "    input_size=784,\n",
        "    hidden_size_list=[100, 100, 100],\n",
        "    output_size=10,\n",
        "    use_dropout=True,\n",
        "    dropout_ratio=0.5,\n",
        "    weight_init_std='xavier',\n",
        "    activation='relu',  # sigmoid 였다면 다를까요?\n",
        ")\n",
        "\n",
        "# 옵티마이저\n",
        "optimizer = SGD(lr=0.01)\n",
        "\n",
        "# 모델 학습 및 검증 시작\n",
        "trainer_xavier = Trainer(\n",
        "    X_train[:200], y_train[:200], # 과적합을 발생시키기 위해 모델에 비해 데이터셋이 부족하게 설정\n",
        "    X_valid, y_valid,\n",
        "    model=model,\n",
        "    optimizer=optimizer,\n",
        "    max_iterations=2000,\n",
        "    batch_size=128,\n",
        ")\n",
        "trainer_xavier.run()\n",
        "\n",
        "# 결과 시각화\n",
        "trainer_xavier.show_results()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CL-UPQy_FVno"
      },
      "source": [
        "#### 👨‍💻 <font color='green'><b>[ 코드 ]</b></font> He 초기화 방법으로 학습하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-08-27T16:17:06.748547Z",
          "start_time": "2023-08-27T16:16:50.646178Z"
        },
        "id": "1ob18yc8FVno"
      },
      "outputs": [],
      "source": [
        "# 모델\n",
        "model = Net(\n",
        "    input_size=784,\n",
        "    hidden_size_list=[100, 100, 100],\n",
        "    output_size=10,\n",
        "    use_dropout=True,\n",
        "    dropout_ratio=0.5,\n",
        "    weight_init_std='he',\n",
        "    activation='relu', # sigmoid 였다면 다를까요?\n",
        ")\n",
        "\n",
        "# 옵티마이저\n",
        "optimizer = SGD(lr=0.01)\n",
        "\n",
        "# 모델 학습 및 검증 시작\n",
        "trainer_he = Trainer(\n",
        "    X_train[:200], y_train[:200], # 과적합을 발생시키기 위해 모델에 비해 데이터셋이 부족하게 설정\n",
        "    X_valid, y_valid,\n",
        "    model=model,\n",
        "    optimizer=optimizer,\n",
        "    max_iterations=2000,\n",
        "    batch_size=128,\n",
        ")\n",
        "trainer_he.run()\n",
        "\n",
        "# 결과 시각화\n",
        "trainer_he.show_results()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MveDiBCzFVno"
      },
      "source": [
        "#### 📚 <font color='blue'><b>[ 자료 ]</b></font>\n",
        "\n",
        "* [Glorot, X., & Bengio, Y. (2010). Understanding the difficulty of training deep feedforward neural networks.](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf) : Xavier 초기화 방법을 제안한 원본 논문입니다. 한 번 살펴보세요.\n",
        "* [He, K., Zhang, X., Ren, S., & Sun, J. (2015). Delving deep into rectifiers: Surpassing human-level performance on imagenet classification.](https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/He_Delving_Deep_into_ICCV_2015_paper.pdf) : He 초기화 방법을 제안한 원본 논문입니다. ReLU 활성화 함수를 사용할 때 가중치를 초기화하는 방법을 설명합니다.\n",
        "* [CS231n 강의노트의 Weight Initialization 부분](http://cs231n.github.io/neural-networks-2/#init) : 가중치 초기화의 중요성과 다양한 초기화 방법을 이해하기 쉽게 설명합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1oqT6CaFVno"
      },
      "source": [
        "## 3. 배치 정규화 (Batch Normalization)\n",
        "\n",
        "<hr style=\"height:5px;border:none;color:#5F71F7;background-color:#5F71F7\">\n",
        "\n",
        "```\n",
        "💡 목차 개요 : 신경망의 과적합을 방지하기 위한 방법 중 하나인 배치 정규화에 대해서 알아보고, 이를 넘파이를 통해 직접 구현해보는 시간을 가집니다.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUXZRuJmFVno"
      },
      "source": [
        "#### 👨‍💻 <font color='green'><b>[ 코드 ]</b></font> 배치 정규화 구현하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-08-27T16:17:06.759877Z",
          "start_time": "2023-08-27T16:17:06.749845Z"
        },
        "id": "m20eGvxjFVno"
      },
      "outputs": [],
      "source": [
        "# BatchNormalization 클래스: 신경망의 학습을 안정화하고 가속화하는 Batch Normalization 기법을 구현합니다.\n",
        "class BatchNormalization:\n",
        "\n",
        "    # 초기화 함수\n",
        "    def __init__(self, gamma, beta, momentum=0.9, running_mean=None, running_var=None):\n",
        "\n",
        "        # gamma, beta: 학습 가능한 스케일 및 시프트 파라미터\n",
        "        self.gamma = gamma\n",
        "        self.beta = beta\n",
        "\n",
        "        # momentum: 평균 및 분산의 움직이는 평균을 계산할 때 사용되는 모멘텀 값\n",
        "        self.momentum = momentum\n",
        "\n",
        "        # 입력 데이터의 형태를 저장 (예: (batch_size, features))\n",
        "        self.input_shape = None\n",
        "\n",
        "        # 학습 중 아닌 상황(예: 평가)에서 사용할 실행 중 평균 및 분산\n",
        "        self.running_mean = running_mean\n",
        "        self.running_var = running_var\n",
        "\n",
        "        # 역전파 시 사용될 중간 값들\n",
        "        self.batch_size = None\n",
        "        self.xc = None\n",
        "        self.std = None\n",
        "        self.dgamma = None\n",
        "        self.dbeta = None\n",
        "\n",
        "    # 순전파 함수\n",
        "    def forward(self, x, train_flag=True):\n",
        "        self.input_shape = x.shape\n",
        "\n",
        "        # 4D 텐서인 경우 2D로 변경\n",
        "        if x.ndim != 2:\n",
        "            N, C, H, W = x.shape\n",
        "            x = x.reshape(N, -1)\n",
        "\n",
        "        # running_mean 및 running_var 초기화\n",
        "        if self.running_mean is None:\n",
        "            N, D = x.shape\n",
        "            self.running_mean = np.zeros(D)\n",
        "            self.running_var = np.zeros(D)\n",
        "\n",
        "        # 학습 시\n",
        "        if train_flag:\n",
        "\n",
        "            # 현재 배치의 평균 및 분산 계산\n",
        "            mu = x.mean(axis=0)\n",
        "            xc = x - mu\n",
        "            var = np.mean(xc**2, axis=0)\n",
        "            std = np.sqrt(var + 10e-7)\n",
        "            xn = xc / std\n",
        "\n",
        "            # 중간 값들 저장\n",
        "            self.batch_size = x.shape[0]\n",
        "            self.xc = xc\n",
        "            self.xn = xn\n",
        "            self.std = std\n",
        "\n",
        "            # 실행 중 평균 및 분산 업데이트\n",
        "            self.running_mean = self.momentum * self.running_mean + (1-self.momentum) * mu\n",
        "            self.running_var = self.momentum * self.running_var + (1-self.momentum) * var\n",
        "\n",
        "        # 평가 시\n",
        "        else:\n",
        "            xc = x - self.running_mean\n",
        "            xn = xc / ((np.sqrt(self.running_var + 10e-7)))\n",
        "\n",
        "        # 최종 출력 계산\n",
        "        out = self.gamma * xn + self.beta\n",
        "\n",
        "        return out.reshape(*self.input_shape)\n",
        "\n",
        "    # 역전파 함수\n",
        "    def backward(self, dout):\n",
        "\n",
        "        # 4D 텐서인 경우 2D로 변경\n",
        "        if dout.ndim != 2:\n",
        "            N, C, H, W = dout.shape\n",
        "            dout = dout.reshape(N, -1)\n",
        "\n",
        "        # 역전파 계산을 위한 그래디언트들\n",
        "        dbeta = dout.sum(axis=0)\n",
        "        dgamma = np.sum(self.xn * dout, axis=0)\n",
        "        dxn = self.gamma * dout\n",
        "        dxc = dxn / self.std\n",
        "        dstd = -np.sum((dxn * self.xc) / (self.std * self.std), axis=0)\n",
        "        dvar = 0.5 * dstd / self.std\n",
        "        dxc += (2.0 / self.batch_size) * self.xc * dvar\n",
        "        dmu = np.sum(dxc, axis=0)\n",
        "        dx = dxc - dmu / self.batch_size\n",
        "\n",
        "        # 학습 가능한 파라미터들의 그래디언트 저장\n",
        "        self.dgamma = dgamma\n",
        "        self.dbeta = dbeta\n",
        "\n",
        "        return dx.reshape(*self.input_shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjbDnIQFFVno"
      },
      "source": [
        "#### 📝 <font color='orange'><b>[ 설명 ]</b></font>\n",
        "* 배치 정규화는 심층 신경망의 내부에서 활성화값이 특정 분포를 유지하도록 도와 학습을 안정화하고 속도를 빠르게 하는 기법입니다.\n",
        "* 기본 아이디어는 네트워크의 각 층의 활성화값이 너무 크거나 작게 되지 않도록 조정함으로써, 네트워크의 학습을 보다 빠르고 안정적으로 만드는 것입니다.\n",
        "* 학습 시에는 현재 배치의 평균과 분산을 계산하여 정규화를 수행하고, 평가 시에는 학습 동안의 움직이는 평균과 분산을 사용하여 정규화합니다.\n",
        "* `running_mean` 및 `running_var` (즉, 이동 평균과 분산)은 학습 중 배치 정규화의 안정성과 테스트 시의 일관성을 위해 도입되었습니다. 그러나 그것들이 항상 필요하다고는 할 수 없습니다. 필요성은 여러 상황에 따라 달라집니다. `running_mean`과 `running_var` 사용하면, 모델이 학습 중과 테스트 중에 동일한 방식으로 동작하는 것을 보장할 수 있습니다.\n",
        "* 테스트나 추론(inference) 시에는 종종 배치 크기가 1이거나 예측하려는 샘플 수에 따라 다양합니다. 이 경우 각 샘플 또는 작은 배치의 평균과 분산을 계산하는 것은 불안정할 수 있습니다. 이 때 `running_mean`과 `running_var` 학습 중에 축적된 평균적인 정보를 제공하여 추론을 안정화합니다. 물론, 특정 상황에서는 `running_mean`과 `running_var` 없이도 잘 작동하는 경우도 있습니다. 예를 들어, 배치 크기가 충분히 크고 데이터의 분포가 학습 데이터와 테스트 데이터에서 크게 달라지지 않는 경우에는 꼭 필요하지 않을 수 있습니다. 결론적으로, 이동 평균과 분산을 사용하는 것은 일반적인 권장 사항이지만, 항상 필요한 것은 아닙니다. 실험과 실험의 결과에 따라 최적의 방법을 결정하는 것이 좋습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 👨‍💻 <font color='green'><b>[ 코드 ]</b></font> 위에서 구현한 배치 정규화로 학습하기"
      ],
      "metadata": {
        "id": "zYnN_ukHVT7r"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-08-27T16:17:29.435807Z",
          "start_time": "2023-08-27T16:17:06.761170Z"
        },
        "scrolled": false,
        "id": "t4JISuYZFVno"
      },
      "outputs": [],
      "source": [
        "# 모델\n",
        "model = Net(\n",
        "    input_size=784,\n",
        "    hidden_size_list=[100, 100, 100],\n",
        "    output_size=10,\n",
        "    weight_init_std='he',\n",
        "    use_dropout=True,\n",
        "    dropout_ratio=0.5,\n",
        "    use_batchnorm=True,  # 이번에는 배치 정규화를 사용해볼까요?\n",
        ")\n",
        "\n",
        "# 옵티마이저\n",
        "optimizer = SGD(lr=0.01)\n",
        "\n",
        "# 모델 학습 및 검증 시작\n",
        "trainer_batchnorm = Trainer(\n",
        "    X_train[:200], y_train[:200], # 과적합을 발생시키기 위해 모델에 비해 데이터셋이 부족하게 설정\n",
        "    X_valid, y_valid,\n",
        "    model=model,\n",
        "    optimizer=optimizer,\n",
        "    max_iterations=2000,\n",
        "    batch_size=128,\n",
        ")\n",
        "trainer_batchnorm.run()\n",
        "\n",
        "# 결과 시각화\n",
        "trainer_batchnorm.show_results()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06_kb8JUFVnp"
      },
      "source": [
        "#### 📚 <font color='blue'><b>[ 자료 ]</b></font>\n",
        "\n",
        "*  [Ioffe, S., & Szegedy, C. (2015). Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift.](https://arxiv.org/abs/1502.03167) : 배치 정규화가 처음 소개되었던 논문입니다. 한 번 살펴보세요."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VoITiZJ6FVnp"
      },
      "source": [
        "#### 📝 <font color='orange'><b>[ 설명 ]</b></font> 드롭아웃, 가중치 초기화 그리고 배치 정규화 성능 비교하기\n",
        "\n",
        "점점 성능 최적화 기법이 적용되면서 어떻게 성능이 변화하는지 한꺼번에 살펴볼까요? 비교를 위해서 검증 데이터의 오차를 비교해보겠습니다. 드롭아웃만 활용했을 경우보다, 가중치 초기화와 배치 정규화를 활용하는 경우가 더욱 오차가 확연히 줄어든 것을 확인할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-08-27T16:22:18.755641Z",
          "start_time": "2023-08-27T16:22:18.543043Z"
        },
        "id": "49i0foryFVnp"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(16,8))\n",
        "\n",
        "plt.plot(trainer_dropout.valid_loss_list)\n",
        "plt.plot(trainer_he.valid_loss_list)\n",
        "plt.plot(trainer_batchnorm.valid_loss_list)\n",
        "\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Valid Loss')\n",
        "plt.legend(['Dropout', 'Dropout + He init', 'Dropout + He init + BatchNorm'])\n",
        "plt.ylim(0,)\n",
        "\n",
        "plt.title('Valid Loss')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "손실 오차 뿐만 아니라 정확도도 점점 좋아지는 걸 확인할 수 있습니다."
      ],
      "metadata": {
        "id": "cHDJB8l8V3oR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-08-27T16:22:33.221778Z",
          "start_time": "2023-08-27T16:22:32.989134Z"
        },
        "id": "rUn2lmIQFVnp"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(16,8))\n",
        "\n",
        "plt.plot(trainer_dropout.valid_acc_list)\n",
        "plt.plot(trainer_he.valid_acc_list)\n",
        "plt.plot(trainer_batchnorm.valid_acc_list)\n",
        "\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Valid Acc')\n",
        "plt.legend(['Dropout', 'Dropout + He init', 'Dropout + He init + BatchNorm'])\n",
        "plt.ylim(0,1)\n",
        "\n",
        "plt.title('Valid Acc')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExvVKf_qFVnp"
      },
      "source": [
        "## 4. 딥러닝에서의 다양한 옵티마이저(Optimizer)\n",
        "\n",
        "<hr style=\"height:5px;border:none;color:#5F71F7;background-color:#5F71F7\">\n",
        "\n",
        "```\n",
        "💡 목차 개요 : 신경망의 가중치를 업데이트하는 다양한 방법들에 대해서 알아보고, 각 알고리즘을 넘파이로 구현해보는 시간을 갖습니다.\n",
        "```\n",
        "\n",
        "- 4-1. SGD 이해 및 구현\n",
        "- 4-2. Momentum 이해 및 구현\n",
        "- 4-3. Nesterov 이해 및 구현\n",
        "- 4-4. AdaGrad 이해 및 구현\n",
        "- 4-5. RMSProp 이해 및 구현\n",
        "- 4-6. Adam 이해 및 구현"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2B83Km6FVnp"
      },
      "source": [
        "### 4-1. SGD\n",
        "\n",
        "> 세부 목차 개요 : SGD는 딥러닝에서 사용되는 기본적인 옵티마이저입니다. 주어진 파라미터의 기울기($\\frac{\\partial L}{\\partial W}$)에 따라 해당 파라미터 값을 업데이트하는 방식으로 동작합니다. 학습률($\\eta$)은 이 업데이트의 크기를 조절하는 중요한 파라미터로, 적절한 값을 선택하는 것이 중요합니다.\n",
        "\n",
        "$$ W \\leftarrow W - \\eta\\frac{\\partial L}{\\partial W} $$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSUW7VjlFVnp"
      },
      "source": [
        "#### 👨‍💻 <font color='green'><b>[ 코드 ]</b></font> SGD 구현하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ig5c4xuOFVnp"
      },
      "source": [
        "```python\n",
        "class SGD:\n",
        "\n",
        "    # 학습률(learning rate)를 설정합니다.\n",
        "    def __init__(self, lr=0.01):\n",
        "        self.lr = lr  # 학습률 설정\n",
        "\n",
        "    # 파라미터를 업데이트하는 함수\n",
        "    def update(self, params, grads):\n",
        "        \n",
        "        # 각 파라미터에 대해\n",
        "        for key in params.keys():\n",
        "            \n",
        "            # 파라미터 값을 기울기 방향으로 학습률만큼 업데이트합니다.\n",
        "            params[key] -= self.lr * grads[key]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GZjFcWIFVnp"
      },
      "source": [
        "#### 📝 <font color='orange'><b>[ 설명 ]</b></font>\n",
        "* 한 번의 업데이트에 하나의 데이터 또는 미니배치를 사용합니다.\n",
        "* SGD는 각 업데이트마다 다른 데이터 또는 미니배치를 사용하므로 \"확률적\"이라는 이름이 붙습니다.\n",
        "* 기본적인 방식이기 때문에 다른 최적화 알고리즘보다 수렴 속도가 느릴 수 있습니다.\n",
        "* 각 파라미터(가중치 및 편향)의 기울기(gradient)를 계산합니다.\n",
        "* 파라미터를 기울기의 반대 방향으로 학습률만큼 조정하여 신경망의 오차를 줄입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 👨‍💻 <font color='green'><b>[ 코드 ]</b></font> 위에서 구현한 SGD 옵티마이저로 학습하기"
      ],
      "metadata": {
        "id": "oRtAPKMCWKhI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-08-27T16:17:52.255502Z",
          "start_time": "2023-08-27T16:17:29.437180Z"
        },
        "id": "7zkJWUtXFVnp"
      },
      "outputs": [],
      "source": [
        "# 모델\n",
        "model = Net(\n",
        "    input_size=784,\n",
        "    hidden_size_list=[100, 100, 100],\n",
        "    output_size=10,\n",
        "    use_batchnorm=True,\n",
        "    weight_init_std='he',\n",
        "    use_dropout=True,\n",
        "    dropout_ratio=0.5,\n",
        ")\n",
        "\n",
        "# 옵티마이저\n",
        "optimizer = SGD(lr=0.01)\n",
        "\n",
        "# 모델 학습 및 검증 시작\n",
        "trainer_sgd = Trainer(\n",
        "    X_train, y_train,\n",
        "    X_valid, y_valid,\n",
        "    model=model,\n",
        "    optimizer=optimizer,\n",
        "    max_iterations=2000,\n",
        "    batch_size=128,\n",
        ")\n",
        "trainer_sgd.run()\n",
        "\n",
        "# 결과 시각화\n",
        "trainer_sgd.show_results()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fhCtVluQFVnp"
      },
      "source": [
        "#### 📚 <font color='blue'><b>[ 자료 ]</b></font>\n",
        "* [\"Deep Learning\" by Goodfellow, Bengio, and Courville]() : 심층 학습에 대한 권위있는 교재로, SGD와 다른 최적화 방법들에 대한 자세한 설명이 있습니다.\n",
        "* [CS231n 강의노트의 Optimization 부분](http://cs231n.github.io/neural-networks-3/#update) : SGD와 다른 최적화 방법들을 비교하며 각각의 장단점에 대해 설명합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOuNgn3_FVnp"
      },
      "source": [
        "### 4-2. 모멘텀 (Momentum)\n",
        "\n",
        "> 세부 목차 개요 : 모멘텀은 \"운동량\" 개념을 SGD에 추가하여 파라미터 업데이트의 방향성을 강화하고, 수렴 속도를 개선하는 옵티마이저입니다. 기존의 기울기 정보($v$)와 현재 기울기 정보($\\frac{\\partial L}{\\partial W}$)를 모두 활용하여 파라미터를 업데이트합니다.\n",
        "\n",
        "$$ v \\leftarrow \\alpha v - \\eta \\frac{\\partial L}{\\partial W} $$\n",
        "\n",
        "$$ W \\leftarrow W + v $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nYQDDSlFVnq"
      },
      "source": [
        "#### 👨‍💻 <font color='green'><b>[ 코드 ]</b></font> 모멘텀 구현하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-08-27T16:17:52.261003Z",
          "start_time": "2023-08-27T16:17:52.256942Z"
        },
        "id": "if1vWW5iFVnq"
      },
      "outputs": [],
      "source": [
        "class Momentum:\n",
        "\n",
        "    # 생성자: 학습률(learning rate) 및 모멘텀 값 설정\n",
        "    def __init__(self, lr=0.01, momentum=0.9):\n",
        "        self.lr = lr                 # 학습률 설정\n",
        "        self.momentum = momentum     # 모멘텀 설정\n",
        "        self.v = None                # 속도 초기화\n",
        "\n",
        "    # 파라미터 업데이트 함수\n",
        "    def update(self, params, grads):\n",
        "\n",
        "        # 첫 번째 호출 시 속도(v)를 파라미터와 동일한 형상의 0으로 초기화\n",
        "        if self.v is None:\n",
        "            self.v = {}\n",
        "            for key, val in params.items():\n",
        "                self.v[key] = np.zeros_like(val)\n",
        "\n",
        "        # 모든 파라미터에 대해\n",
        "        for key in params.keys():\n",
        "\n",
        "            # 속도 업데이트\n",
        "            self.v[key] = self.momentum * self.v[key] - self.lr * grads[key]\n",
        "\n",
        "            # 파라미터 업데이트\n",
        "            params[key] += self.v[key]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1OlRFz4FVnq"
      },
      "source": [
        "#### 📝 <font color='orange'><b>[ 설명 ]</b></font>\n",
        "*  속도(v)는 과거의 기울기를 어느 정도 기억하면서 업데이트에 활용됩니다.\n",
        "* 이로 인해 기울기가 지속적으로 같은 방향으로 움직이면, 그 방향으로 가속도가 붙게 됩니다. 반대로, 기울기가 방향을 바꾸면 속도는 감소합니다.\n",
        "* 이러한 동작으로 인해 모멘텀은 SGD보다 더 빠르게 수렴하며, 지역 최솟값(local minima)에서 벗어나는 데도 효과적입니다.\n",
        "* 파라미터의 기울기에 학습률을 곱한 값을 현재 속도에 더하고, 이 값을 파라미터에서 뺍니다.\n",
        "* v는 현재 기울기와 이전 단계에서의 속도를 모두 고려하여 파라미터를 얼마나 업데이트할지 결정하는데 사용됩니다.\n",
        "* 모멘텀 값은 이전 단계의 속도에 대한 가중치로, [0, 1] 사이의 값을 가집니다. 일반적으로 0.9와 같은 높은 값을 사용합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 👨‍💻 <font color='green'><b>[ 코드 ]</b></font> 위에서 구현한 모멘텀 옵티마이저로 학습하기\n"
      ],
      "metadata": {
        "id": "sczJmZAgWQU1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-08-27T16:18:15.611843Z",
          "start_time": "2023-08-27T16:17:52.262283Z"
        },
        "id": "van5O5T3FVnq"
      },
      "outputs": [],
      "source": [
        "# 모델\n",
        "model = Net(\n",
        "    input_size=784,\n",
        "    hidden_size_list=[100, 100, 100],\n",
        "    output_size=10,\n",
        "    use_batchnorm=True,\n",
        "    weight_init_std='he',\n",
        "    use_dropout=True,\n",
        "    dropout_ratio=0.5,\n",
        ")\n",
        "\n",
        "# 옵티마이저\n",
        "optimizer = Momentum(lr=0.01, momentum=0.9)\n",
        "\n",
        "# 모델 학습 및 검증 시작\n",
        "trainer_momentum = Trainer(\n",
        "    X_train, y_train,\n",
        "    X_valid, y_valid,\n",
        "    model=model,\n",
        "    optimizer=optimizer,\n",
        "    max_iterations=2000,\n",
        "    batch_size=128,\n",
        ")\n",
        "trainer_momentum.run()\n",
        "\n",
        "# 결과 시각화\n",
        "trainer_momentum.show_results()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptUb0U75FVnq"
      },
      "source": [
        "#### 📚 <font color='blue'><b>[ 자료 ]</b></font>\n",
        "* [\"Deep Learning\" by Goodfellow, Bengio, and Courville]() : 심층 학습에 대한 권위있는 교재로, SGD와 다른 최적화 방법들에 대한 자세한 설명이 있습니다.\n",
        "* [CS231n 강의노트의 Optimization 부분](http://cs231n.github.io/neural-networks-3/#update) : SGD와 다른 최적화 방법들을 비교하며 각각의 장단점에 대해 설명합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEIhcT_-FVnq"
      },
      "source": [
        "### 4-3. Nesterov\n",
        "\n",
        "> 세부 목차 개요 : Nesterov Accelerated Gradient (NAG)는 모멘텀 방식을 향상시킨 최적화 방법입니다. 기본 아이디어는 \"미리보기\"를 사용하여 미래의 파라미터 위치에서의 기울기를 계산하고 이를 사용하여 파라미터를 업데이트하는 것입니다. 이로 인해, 최적화 과정이 더 부드럽게 이루어지며 수렴 속도가 개선됩니다.\n",
        "\n",
        "$$ v \\leftarrow \\alpha v - \\eta \\frac{\\partial L}{\\partial W}(W + \\alpha v)$$\n",
        "\n",
        "$$ W \\leftarrow W + v $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "niZVKUQaFVnq"
      },
      "source": [
        "#### 👨‍💻 <font color='green'><b>[ 코드 ]</b></font> Nesterov 구현하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-08-27T16:18:15.617729Z",
          "start_time": "2023-08-27T16:18:15.613187Z"
        },
        "id": "rLxETlPSFVnq"
      },
      "outputs": [],
      "source": [
        "class Nesterov:\n",
        "\n",
        "    # 학습률(learning rate) 및 모멘텀 계수 설정\n",
        "    def __init__(self, lr=0.01, momentum=0.9):\n",
        "        self.lr = lr             # 학습률\n",
        "        self.momentum = momentum # 모멘텀 계수\n",
        "        self.v = None            # 속도\n",
        "\n",
        "    # 파라미터 업데이트 함수\n",
        "    def update(self, params, grads):\n",
        "        if self.v is None:\n",
        "            self.v = {}\n",
        "            for key, val in params.items():\n",
        "                self.v[key] = np.zeros_like(val)\n",
        "\n",
        "        # 각 파라미터에 대해\n",
        "        for key in params.keys():\n",
        "\n",
        "            # 현재 속도를 기반으로 예측된 위치 계산\n",
        "            w_pred = params[key] + self.momentum * self.v[key]\n",
        "\n",
        "            # 예측된 위치에서의 그래디언트 계산\n",
        "            # 여기서는 간단하게 현재 위치에서의 그래디언트를 사용합니다.\n",
        "            g = grads[key]\n",
        "\n",
        "            # 속도 업데이트\n",
        "            self.v[key] = self.momentum * self.v[key] - self.lr * g\n",
        "\n",
        "            # 파라미터 업데이트\n",
        "            params[key] += self.v[key]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIOcYE-ZFVnq"
      },
      "source": [
        "#### 📝 <font color='orange'><b>[ 설명 ]</b></font>\n",
        "* 기본 아이디어는 파라미터가 업데이트될 방향을 \"미리 살펴보는\" 것입니다.\n",
        "* 다시 말해, 모멘텀에 따라 미래의 파라미터 위치를 \"예측\"하고 그 위치에서의 기울기를 사용하여 파라미터를 업데이트합니다.\n",
        "* Nesterov 모멘텀은 먼저 모멘텀만으로 파라미터를 임시로 이동시키고 (이동된 임시 위치를 \"미리보기\" 위치로 생각할 수 있음), 이동된 위치에서의 기울기를 계산하여 파라미터를 업데이트합니다.\n",
        "* 이 방식은 파라미터 업데이트가 더 부드럽게 이루어지도록 도와줍니다."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 👨‍💻 <font color='green'><b>[ 코드 ]</b></font> 위에서 구현한 Nesterov 옵티마이저로 학습하기\n"
      ],
      "metadata": {
        "id": "Ypdkgii4WSCu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-08-27T16:18:39.570983Z",
          "start_time": "2023-08-27T16:18:15.619027Z"
        },
        "id": "F8y5B0AxFVnq"
      },
      "outputs": [],
      "source": [
        "# 모델\n",
        "model = Net(\n",
        "    input_size=784,\n",
        "    hidden_size_list=[100, 100, 100],\n",
        "    output_size=10,\n",
        "    use_batchnorm=True,\n",
        "    weight_init_std='he',\n",
        "    use_dropout=True,\n",
        "    dropout_ratio=0.5,\n",
        ")\n",
        "\n",
        "# 옵티마이저\n",
        "optimizer = Nesterov(lr=0.01, momentum=0.9)\n",
        "\n",
        "# 모델 학습 및 검증 시작\n",
        "trainer_nesterov = Trainer(\n",
        "    X_train, y_train,\n",
        "    X_valid, y_valid,\n",
        "    model=model,\n",
        "    optimizer=optimizer,\n",
        "    max_iterations=2000,\n",
        "    batch_size=128,\n",
        ")\n",
        "trainer_nesterov.run()\n",
        "\n",
        "# 결과 시각화\n",
        "trainer_nesterov.show_results()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D59xGiacFVnq"
      },
      "source": [
        "#### 📚 <font color='blue'><b>[ 자료 ]</b></font>\n",
        "\n",
        "* [\"A method for unconstrained convex minimization problem with the rate of convergence O(1/k^2)\" by Yurii Nesterov](https://) : Nesterov 모멘텀 방식을 처음 제안한 논문입니다.\n",
        "* [\"Why Momentum Really Works\" by Gabriel Goh](https://distill.pub/2017/momentum/) : 모멘텀과 Nesterov 모멘텀의 직관적 이해를 도와주는 그림과 설명이 포함되어 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hep74ohjFVnq"
      },
      "source": [
        "### 4-4. AdaGrad\n",
        "\n",
        "> 세부 목차 개요 : AdaGrad는 각 매개변수에 대해 학습률($\\eta$)을 개별적으로 조절하는 최적화 알고리즘입니다. 자주 갱신되는 매개변수는 학습률을 점차 줄이고, 자주 갱신되지 않는 매개변수는 학습률을 크게 유지합니다. $h$의 크기가 클수록 적게 갱신되고 작을수록 크게 갱신됩니다. 이를 통해 매개변수마다 적절한 학습률을 자동으로 설정해 줍니다.\n",
        "\n",
        "$$ h \\leftarrow h + \\frac{\\partial L}{\\partial W} \\odot \\frac{\\partial L}{\\partial W} $$\n",
        "\n",
        "$$ W \\leftarrow W - \\eta \\frac{1}{\\sqrt{h}}\\frac{\\partial L}{\\partial W} $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6mEHgbJFVnq"
      },
      "source": [
        "#### 👨‍💻 <font color='green'><b>[ 코드 ]</b></font> AdaGrad 구현하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-08-27T16:18:39.580409Z",
          "start_time": "2023-08-27T16:18:39.572962Z"
        },
        "id": "rgJA4JbwFVnq"
      },
      "outputs": [],
      "source": [
        "class AdaGrad:\n",
        "\n",
        "    # 학습률(learning rate) 설정\n",
        "    def __init__(self, lr=0.01):\n",
        "        self.lr = lr      # 학습률 설정\n",
        "        self.h = None     # 이전 기울기의 제곱 합을 저장할 변수 초기화\n",
        "\n",
        "    # 파라미터 업데이트 함수\n",
        "    def update(self, params, grads):\n",
        "\n",
        "        # 첫 번째 호출 시 h를 파라미터와 동일한 형상의 0으로 초기화\n",
        "        if self.h is None:\n",
        "            self.h = {}\n",
        "            for key, val in params.items():\n",
        "                self.h[key] = np.zeros_like(val)\n",
        "\n",
        "        # 각 파라미터에 대해\n",
        "        for key in params.keys():\n",
        "\n",
        "            # 기울기의 제곱 합을 h에 누적\n",
        "            self.h[key] += grads[key] * grads[key]\n",
        "\n",
        "            # 파라미터 업데이트 (AdaGrad 특징 부분)\n",
        "            # 0으로 나누는 것을 방지하기 위한 작은 상수 추가\n",
        "            params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0xrTVswFVnq"
      },
      "source": [
        "#### 📝 <font color='orange'><b>[ 설명 ]</b></font>\n",
        "* 자주 등장하거나 큰 변화를 보이는 매개변수의 학습률은 작게 만들고, 그렇지 않은 매개변수는 학습률을 크게 만듭니다. 이는 학습률 감소(learning rate decay) 기법 중 하나로, 각 매개변수에 맞춤형 값을 만들어줍니다.\n",
        "* 기울기의 제곱합을 $h$에 누적시킨다. 매개변수를 업데이트할 때 $\\frac{1}{\\sqrt{h}}$를 곱해 학습률을 조절한다. 따라서, 학습을 진행하면서 갱신 강도가 약해진다.\n",
        "* 주의해야할 점은 학습을 계속하면 갱신 강도가 계속 약해져서, 무한히 계속 학습한다면 어느 순간 갱신량이 0이 되어 전혀 갱신되지 않게 됩니다."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 👨‍💻 <font color='green'><b>[ 코드 ]</b></font> 위에서 구현한 AdaGrad 옵티마이저로 학습하기\n"
      ],
      "metadata": {
        "id": "Fpd_9HLgWVIb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-08-27T16:19:05.095921Z",
          "start_time": "2023-08-27T16:18:39.582647Z"
        },
        "id": "YDl_lsz2FVnq"
      },
      "outputs": [],
      "source": [
        "# 모델\n",
        "model = Net(\n",
        "    input_size=784,\n",
        "    hidden_size_list=[100, 100, 100],\n",
        "    output_size=10,\n",
        "    use_batchnorm=True,\n",
        "    weight_init_std='he',\n",
        "    use_dropout=True,\n",
        "    dropout_ratio=0.5,\n",
        ")\n",
        "\n",
        "# 옵티마이저\n",
        "optimizer = AdaGrad(lr=0.01)\n",
        "\n",
        "# 모델 학습 및 검증 시작\n",
        "trainer_adagrad = Trainer(\n",
        "    X_train, y_train,\n",
        "    X_valid, y_valid,\n",
        "    model=model,\n",
        "    optimizer=optimizer,\n",
        "    max_iterations=2000,\n",
        "    batch_size=128,\n",
        ")\n",
        "trainer_adagrad.run()\n",
        "\n",
        "# 결과 시각화\n",
        "trainer_adagrad.show_results()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQij0wj3FVnr"
      },
      "source": [
        "#### 📚 <font color='blue'><b>[ 자료 ]</b></font>\n",
        "\n",
        "* [\"Adaptive Subgradient Methods for Online Learning and Stochastic Optimization\" by John Duchi, Elad Hazan, and Yoram Singer.](https://) : AdaGrad 알고리즘을 처음 소개한 논문입니다.\n",
        "* [\"Deep Learning\" by Goodfellow et al.]() : AdaGrad와 관련된 자세한 설명이 포함되어 있습니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5FRyGRaFVnr"
      },
      "source": [
        "### 4-5. RMSProp\n",
        "\n",
        "> 세부 목차 개요 : RMSprop는 각 매개변수에 대해 학습률을 개별적으로 조절하는 최적화 알고리즘이다. AdaGrad의 문제를 해결하기 위해, 최근 기울기만($\\beta$로 계산) 크게 반영하도록 설계되었다. 이를 통해 학습률이 너무 빠르게 감소하는 문제를 방지하며, 안정적으로 학습을 진행할 수 있다.\n",
        "\n",
        "$$ h \\leftarrow \\beta h + (1-\\beta) (\\frac{\\partial L}{\\partial W} \\odot \\frac{\\partial L}{\\partial W}) $$\n",
        "\n",
        "$$ W \\leftarrow W - \\eta \\frac{1}{\\sqrt{h+\\epsilon}} \\odot \\frac{\\partial L}{\\partial W} $$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BivfFkU5FVnr"
      },
      "source": [
        "#### 👨‍💻 <font color='green'><b>[ 코드 ]</b></font> RMSProp 구현하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-08-27T16:19:05.105968Z",
          "start_time": "2023-08-27T16:19:05.098140Z"
        },
        "id": "AvZaXYMLFVnr"
      },
      "outputs": [],
      "source": [
        "class RMSprop:\n",
        "\n",
        "    # 학습률(learning rate)와 감쇠율(decay rate) 설정\n",
        "    def __init__(self, lr=0.01, decay_rate=0.99):\n",
        "        self.lr = lr                  # 학습률 설정\n",
        "        self.decay_rate = decay_rate  # 감쇠율 설정\n",
        "        self.h = None                 # 이전 기울기의 제곱 합의 이동 평균을 저장할 변수 초기화\n",
        "\n",
        "    # 파라미터 업데이트 함수\n",
        "    def update(self, params, grads):\n",
        "\n",
        "        # 첫 번째 호출 시 h를 파라미터와 동일한 형상의 0으로 초기화\n",
        "        if self.h is None:\n",
        "            self.h = {}\n",
        "            for key, val in params.items():\n",
        "                self.h[key] = np.zeros_like(val)\n",
        "\n",
        "        # 각 파라미터에 대해\n",
        "        for key in params.keys():\n",
        "\n",
        "            # h를 감쇠율로 감소시키고\n",
        "            self.h[key] *= self.decay_rate\n",
        "\n",
        "            # 기울기의 제곱의 (1-감쇠율) 비율을 더한다.\n",
        "            self.h[key] += (1 - self.decay_rate) * grads[key] * grads[key]\n",
        "\n",
        "            # 파라미터 업데이트 (RMSprop 특징 부분)\n",
        "            # 0으로 나누는 것을 방지하기 위한 작은 상수 추가\n",
        "            params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krkhJYudFVnr"
      },
      "source": [
        "#### 📝 <font color='orange'><b>[ 설명 ]</b></font>\n",
        "* AdaGrad의 학습률 감소 문제를 해결하기 위한 방법 중 하나로 제안되었습니다.\n",
        "* 과거의 모든 기울기를 균일하게 더하는 대신, 더 최근의 기울기를 크게 반영하는 지수 이동 평균을 사용합니다. 이 방법을 통해, RMSprop는 너무 빠른 학습률의 감소를 방지하면서도 안정적인 학습을 가능하게 합니다.\n",
        "* $h$는 기울기의 제곱의 지수 이동 평균으로 갱신된다. 학습률을 $\\frac{1}{\\sqrt{h}}$를 사용해 조절합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 👨‍💻 <font color='green'><b>[ 코드 ]</b></font> 위에서 구현한 RMSProp 옵티마이저로 학습하기\n"
      ],
      "metadata": {
        "id": "TT4V5GnNWYnu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-08-27T16:23:15.317808Z",
          "start_time": "2023-08-27T16:22:49.436358Z"
        },
        "id": "KBpW71cjFVnr"
      },
      "outputs": [],
      "source": [
        "# 모델\n",
        "model = Net(\n",
        "    input_size=784,\n",
        "    hidden_size_list=[100, 100, 100],\n",
        "    output_size=10,\n",
        "    use_batchnorm=True,\n",
        "    weight_init_std='he',\n",
        "    use_dropout=True,\n",
        "    dropout_ratio=0.5,\n",
        ")\n",
        "\n",
        "# 옵티마이저\n",
        "optimizer = RMSprop(lr=0.01, decay_rate=0.99)\n",
        "\n",
        "# 모델 학습 및 검증 시작\n",
        "trainer_rmsprop = Trainer(\n",
        "    X_train, y_train,\n",
        "    X_valid, y_valid,\n",
        "    model=model,\n",
        "    optimizer=optimizer,\n",
        "    max_iterations=2000,\n",
        "    batch_size=128,\n",
        ")\n",
        "trainer_rmsprop.run()\n",
        "\n",
        "# 결과 시각화\n",
        "trainer_rmsprop.show_results()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6FUO_kLfFVnr"
      },
      "source": [
        "#### 📚 <font color='blue'><b>[ 자료 ]</b></font>\n",
        "* [Geoffrey Hinton의 Coursera 강의 \"Neural Networks for Machine Learning\".](https://) : RMSprop 알고리즘이 처음 소개합니다. 한번 살펴보세요.\n",
        "* [\"Deep Learning\" by Goodfellow et al.]() : AdaGrad와 관련된 자세한 설명이 포함되어 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZBeQKmSFVnr"
      },
      "source": [
        "### 4-6. Adam\n",
        "\n",
        "> 세부 목차 개요 : Adam은 모멘텀과 RMSprop의 아이디어를 결합한 최적화 알고리즘입니다. 기울기의 지수 이동 평균과 기울기의 제곱의 지수 이동 평균을 동시에 추정하며, 바이어스 보정을 사용하여 초기 단계에서의 계산 바이어스를 제거합니다.\n",
        "\n",
        "$$ m \\leftarrow \\beta_{1}m + (1-\\beta_{1})\\frac{\\partial L}{\\partial W} $$\n",
        "\n",
        "$$ v \\leftarrow \\beta_{2}v + (1-\\beta_{2})(\\frac{\\partial L}{\\partial W})^2 $$\n",
        "\n",
        "$$ W \\leftarrow W - \\eta \\frac{m}{\\sqrt{v} + \\epsilon} $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7WiZuS-FVnr"
      },
      "source": [
        "#### 👨‍💻 <font color='green'><b>[ 코드 ]</b></font> Adam 구현하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-08-27T16:23:15.428989Z",
          "start_time": "2023-08-27T16:23:15.422328Z"
        },
        "id": "kXwFQvGzFVnr"
      },
      "outputs": [],
      "source": [
        "class Adam:\n",
        "\n",
        "    # Adam의 기본 하이퍼파라미터들을 초기화\n",
        "    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999):\n",
        "        self.lr = lr       # 학습률\n",
        "        self.beta1 = beta1 # 모멘텀에 사용되는 계수\n",
        "        self.beta2 = beta2 # RMSprop에 사용되는 계수\n",
        "        self.iter = 0      # 반복 횟수 저장용\n",
        "        self.m = None      # 1차 모멘텀용 누적 값\n",
        "        self.v = None      # 2차 모멘텀용 누적 값 (제곱된 기울기의 이동 평균)\n",
        "\n",
        "    # 파라미터 업데이트 함수\n",
        "    def update(self, params, grads):\n",
        "\n",
        "        # 첫 번째 호출 시 m, v 초기화\n",
        "        if self.m is None:\n",
        "            self.m, self.v = {}, {}\n",
        "            for key, val in params.items():\n",
        "                self.m[key] = np.zeros_like(val)\n",
        "                self.v[key] = np.zeros_like(val)\n",
        "\n",
        "        # 반복 횟수 증가\n",
        "        self.iter += 1\n",
        "\n",
        "        # 학습률의 바이어스 보정 (bias correction)\n",
        "        # 이는 초기 불안정한 학습을 안정적으로 도와줌\n",
        "        lr_t  = self.lr * np.sqrt(1.0 - self.beta2**self.iter) / (1.0 - self.beta1**self.iter)\n",
        "\n",
        "        # 각 파라미터에 대해\n",
        "        for key in params.keys():\n",
        "\n",
        "            # 1차 모멘텀 계산\n",
        "            self.m[key] += (1 - self.beta1) * (grads[key] - self.m[key])\n",
        "\n",
        "            # 2차 모멘텀 (제곱된 기울기의 이동 평균) 계산\n",
        "            self.v[key] += (1 - self.beta2) * (grads[key]**2 - self.v[key])\n",
        "\n",
        "            # 파라미터 업데이트\n",
        "            params[key] -= lr_t * self.m[key] / (np.sqrt(self.v[key]) + 1e-7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TS-OnvAjFVnr"
      },
      "source": [
        "#### 📝 <font color='orange'><b>[ 설명 ]</b></font>\n",
        "* 첫 번째 모멘텀(m)은 과거 기울기의 지수 이동 평균을 추정합니다.\n",
        "* 두 번째 모멘텀(v)는 과거 기울기의 제곱의 지수 이동 평균을 추정합니다.\n",
        "* Adam은 바이어스 보정을 사용하여 초반의 계산에서 생기는 작은 값의 바이어스를 제거합니다.\n",
        "* m은 기울기의 지수 이동 평균으로 갱신된다.\n",
        "* v는 기울기의 제곱의 지수 이동 평균으로 갱신된다.\n",
        "* 학습률의 바이어스 보정을 통해 각각의 매개변수 업데이트에 적절한 학습률을 적용한다."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 👨‍💻 <font color='green'><b>[ 코드 ]</b></font> 위에서 구현한 Adam 옵티마이저로 학습하기\n"
      ],
      "metadata": {
        "id": "1Yu991A1Wcu2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-08-27T16:25:20.455728Z",
          "start_time": "2023-08-27T16:24:52.947130Z"
        },
        "id": "NKWVUCGhFVnr"
      },
      "outputs": [],
      "source": [
        "# 모델\n",
        "model = Net(\n",
        "    input_size=784,\n",
        "    hidden_size_list=[100, 100, 100],\n",
        "    output_size=10,\n",
        "    use_batchnorm=True,\n",
        "    weight_init_std='he',\n",
        "    use_dropout=True,\n",
        "    dropout_ratio=0.5,\n",
        ")\n",
        "\n",
        "# 옵티마이저\n",
        "optimizer = Adam(lr=0.001, beta1=0.9, beta2=0.999)\n",
        "\n",
        "# 모델 학습 및 검증 시작\n",
        "trainer_adam = Trainer(\n",
        "    X_train, y_train,\n",
        "    X_valid, y_valid,\n",
        "    model=model,\n",
        "    optimizer=optimizer,\n",
        "    max_iterations=2000,\n",
        "    batch_size=128,\n",
        ")\n",
        "trainer_adam.run()\n",
        "\n",
        "# 결과 시각화\n",
        "trainer_adam.show_results()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmMRtHQKFVnr"
      },
      "source": [
        "#### 📚 <font color='blue'><b>[ 자료 ]</b></font>\n",
        "* [Adam: A Method for Stochastic Optimization](https://arxiv.org/abs/1412.6980) : Adam 알고리즘이 처음 소개된 원본 논문입니다. 자세한 수식과 이론적 배경을 참조하실 수 있습니다.\n",
        "* [\"Deep Learning\" by Goodfellow et al.]() : Adam 외에도 다양한 최적화 방법에 대한 자세한 설명이 포함되어 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RTD7yR2FVnr"
      },
      "source": [
        "#### 📝 <font color='orange'><b>[ 설명 ]</b></font> SGD부터 Adam 비교하기\n",
        "\n",
        "다양한 최적화 기법에 따라 어떻게 성능이 달라지는지 살펴볼까요? 잊지 말고 꼭 기억해야할 부분은 \"어느 상황에서든 항상 좋은\" 옵티마이저는 없다는 것입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-08-27T16:25:20.777740Z",
          "start_time": "2023-08-27T16:25:20.559144Z"
        },
        "id": "oorOJPG0FVnr"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(16,8))\n",
        "\n",
        "plt.plot(trainer_sgd.valid_loss_list)\n",
        "plt.plot(trainer_momentum.valid_loss_list)\n",
        "plt.plot(trainer_nesterov.valid_loss_list)\n",
        "plt.plot(trainer_adagrad.valid_loss_list)\n",
        "plt.plot(trainer_rmsprop.valid_loss_list)\n",
        "plt.plot(trainer_adam.valid_loss_list)\n",
        "\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Valid Loss')\n",
        "plt.legend(['SGD', 'Momentum', 'Nesterov', 'AdaGrad', 'RMSProp', 'Adam'])\n",
        "plt.ylim(0,)\n",
        "\n",
        "plt.title('Valid Loss')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-08-27T16:25:21.159116Z",
          "start_time": "2023-08-27T16:25:20.880720Z"
        },
        "id": "kiVotqdxFVnr"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(16,8))\n",
        "\n",
        "plt.plot(trainer_sgd.valid_acc_list)\n",
        "plt.plot(trainer_momentum.valid_acc_list)\n",
        "plt.plot(trainer_nesterov.valid_acc_list)\n",
        "plt.plot(trainer_adagrad.valid_acc_list)\n",
        "plt.plot(trainer_rmsprop.valid_acc_list)\n",
        "plt.plot(trainer_adam.valid_acc_list)\n",
        "\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Valid Acc')\n",
        "plt.legend(['SGD', 'Momentum', 'Nesterov', 'AdaGrad', 'RMSProp', 'Adam'])\n",
        "plt.ylim(0,1)\n",
        "\n",
        "plt.title('Valid Acc')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGetJr8AFVns"
      },
      "source": [
        "## Reference\n",
        "\n",
        "<hr style=\"height:5px;border:none;color:#5F71F7;background-color:#5F71F7\">\n",
        "\n",
        "- [MNIST 데이터셋 이미지 출처](https://upload.wikimedia.org/wikipedia/commons/f/f7/MnistExamplesModified.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Required Package\n",
        "\n",
        "numpy version >= 1.23.5 <br>\n",
        "matplotlib version >= 3.7.1"
      ],
      "metadata": {
        "id": "mBOc3AS_XGIn"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5Oa7hzfFVns"
      },
      "source": [
        "## 콘텐츠 라이선스\n",
        "\n",
        "저작권 : <font color='blue'> <b> ©2023 by Upstage X fastcampus Co., Ltd. All rights reserved.</font></b>\n",
        "\n",
        "<font color='red'><b>WARNING</font> : 본 교육 콘텐츠의 지식재산권은 업스테이지 및 패스트캠퍼스에 귀속됩니다. 본 콘텐츠를 어떠한 경로로든 외부로 유출 및 수정하는 행위를 엄격히 금합니다. </b>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J1SnLMCqhcgZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": false,
      "sideBar": true,
      "skip_h1_title": true,
      "title_cell": "실습 목차",
      "title_sidebar": "실습 목차",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}